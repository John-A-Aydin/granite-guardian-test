{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Granite Guardian : Quick Start Guide\n",
    "\n",
    "Links to ðŸ¤— models: [8B](https://huggingface.co/ibm-granite/granite-guardian-3.3-8b)\n",
    "\n",
    "<span style=\"color: red;\">Content Warning</span>: *The examples used in this page may contain offensive language, stereotypes, or discriminatory content.*\n",
    "\n",
    "\n",
    "## What's new? âœ¨\n",
    "* _Hybrid Reasoning Model_: Tackle complex tasks with new hybrid reasoning model. Users can now toggle reasoning **on** or **off** (via `think` flag) to best suit their needs. \n",
    "* _Enchanced Performance_: We have made significant improvements in groundedness and function-call hallucination detection capabilities. ðŸš€ \n",
    "* _Updated Customization_: **'risk'** is now **'criteria'** to better fit a wide variety of use cases. ðŸŽ¯ And with new *'Bring Your Own Criteria'* feature, users can provide their own custom criteria and scoring schema, giving them power to customize the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install vllm torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/granite_guardian/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 36 key-value pairs and 362 tensors from ../model_gguf/granite-guardian-3.3-8b-Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = granite\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Granite Guardian 3.3 8b\n",
      "llama_model_loader: - kv   3:                           general.basename str              = granite-guardian-3.3\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   8:                        granite.block_count u32              = 40\n",
      "llama_model_loader: - kv   9:                     granite.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                   granite.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                granite.feed_forward_length u32              = 12800\n",
      "llama_model_loader: - kv  12:               granite.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:            granite.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                     granite.rope.freq_base f32              = 10000000.000000\n",
      "llama_model_loader: - kv  15:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                         granite.vocab_size u32              = 49159\n",
      "llama_model_loader: - kv  17:               granite.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                    granite.attention.scale f32              = 0.007812\n",
      "llama_model_loader: - kv  19:                    granite.embedding_scale f32              = 12.000000\n",
      "llama_model_loader: - kv  20:                     granite.residual_scale f32              = 0.220000\n",
      "llama_model_loader: - kv  21:                        granite.logit_scale f32              = 16.000000\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = refact\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49159]   = [\"<|end_of_text|>\", \"<fim_prefix>\", \"...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49159]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"Ä Ä Ä Ä  Ä Ä ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- set criteria_bank = ({\\n    \"socia...\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q8_0:  281 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 8.09 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token:  49156 '<|end_of_cite|>' is not marked as EOG\n",
      "load: control token:  49157 '<|start_of_plugin|>' is not marked as EOG\n",
      "load: control token:      2 '<fim_middle>' is not marked as EOG\n",
      "load: control token:     13 '<jupyter_output>' is not marked as EOG\n",
      "load: control token:      9 '<issue_closed>' is not marked as EOG\n",
      "load: control token:      6 '<gh_stars>' is not marked as EOG\n",
      "load: control token:     10 '<jupyter_start>' is not marked as EOG\n",
      "load: control token:     14 '<empty_output>' is not marked as EOG\n",
      "load: control token:     15 '<commit_before>' is not marked as EOG\n",
      "load: control token:      5 '<filename>' is not marked as EOG\n",
      "load: control token:     12 '<jupyter_code>' is not marked as EOG\n",
      "load: control token:      7 '<issue_start>' is not marked as EOG\n",
      "load: control token:      3 '<fim_suffix>' is not marked as EOG\n",
      "load: control token:      1 '<fim_prefix>' is not marked as EOG\n",
      "load: control token:      8 '<issue_comment>' is not marked as EOG\n",
      "load: control token:     11 '<jupyter_text>' is not marked as EOG\n",
      "load: control token:     16 '<commit_msg>' is not marked as EOG\n",
      "load: control token:  49152 '<|start_of_role|>' is not marked as EOG\n",
      "load: control token:  49155 '<|start_of_cite|>' is not marked as EOG\n",
      "load: control token:  49154 '<|tool_call|>' is not marked as EOG\n",
      "load: control token:  49153 '<|end_of_role|>' is not marked as EOG\n",
      "load: control token:     17 '<commit_after>' is not marked as EOG\n",
      "load: control token:  49158 '<|end_of_plugin|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 0 ('<|end_of_text|>')\n",
      "load:   - 4 ('<fim_pad>')\n",
      "load:   - 18 ('<reponame>')\n",
      "load: special tokens cache size = 26\n",
      "load: token to piece cache size = 0.2827 MB\n",
      "print_info: arch             = granite\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 40\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 1.6e+01\n",
      "print_info: f_attn_scale     = 7.8e-03\n",
      "print_info: n_ff             = 12800\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = yes\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 8.17 B\n",
      "print_info: general.name     = Granite Guardian 3.3 8b\n",
      "print_info: f_embedding_scale = 12.000000\n",
      "print_info: f_residual_scale  = 0.220000\n",
      "print_info: f_attention_scale = 0.007812\n",
      "print_info: n_ff_shexp        = 0\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 49159\n",
      "print_info: n_merges         = 48891\n",
      "print_info: BOS token        = 0 '<|end_of_text|>'\n",
      "print_info: EOS token        = 0 '<|end_of_text|>'\n",
      "print_info: UNK token        = 0 '<|end_of_text|>'\n",
      "print_info: PAD token        = 0 '<|end_of_text|>'\n",
      "print_info: LF token         = 203 'ÄŠ'\n",
      "print_info: FIM PRE token    = 1 '<fim_prefix>'\n",
      "print_info: FIM SUF token    = 3 '<fim_suffix>'\n",
      "print_info: FIM MID token    = 2 '<fim_middle>'\n",
      "print_info: FIM PAD token    = 4 '<fim_pad>'\n",
      "print_info: FIM REP token    = 18 '<reponame>'\n",
      "print_info: EOG token        = 0 '<|end_of_text|>'\n",
      "print_info: EOG token        = 4 '<fim_pad>'\n",
      "print_info: EOG token        = 18 '<reponame>'\n",
      "print_info: max token length = 512\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  37 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  38 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  39 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  40 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 362 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  8280.29 MiB\n",
      "...................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 1024\n",
      "llama_context: n_ctx_per_seq = 1024\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (1024) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.19 MiB\n",
      "create_memory: n_ctx = 1024 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified: layer  34: dev = CPU\n",
      "llama_kv_cache_unified: layer  35: dev = CPU\n",
      "llama_kv_cache_unified: layer  36: dev = CPU\n",
      "llama_kv_cache_unified: layer  37: dev = CPU\n",
      "llama_kv_cache_unified: layer  38: dev = CPU\n",
      "llama_kv_cache_unified: layer  39: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   160.00 MiB\n",
      "llama_kv_cache_unified: size =  160.00 MiB (  1024 cells,  40 layers,  1/1 seqs), K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2896\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   105.01 MiB\n",
      "llama_context: graph nodes  = 1488\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.file_type': '7', 'general.quantization_version': '2', 'granite.rope.freq_base': '10000000.000000', 'granite.context_length': '131072', 'tokenizer.ggml.add_space_prefix': 'false', 'granite.attention.head_count': '32', 'granite.vocab_size': '49159', 'granite.embedding_length': '4096', 'granite.attention.scale': '0.007812', 'general.type': 'model', 'granite.attention.head_count_kv': '8', 'granite.embedding_scale': '12.000000', 'general.basename': 'granite-guardian-3.3', 'tokenizer.ggml.padding_token_id': '0', 'granite.feed_forward_length': '12800', 'general.license': 'apache-2.0', 'granite.logit_scale': '16.000000', 'tokenizer.ggml.pre': 'refact', 'general.architecture': 'granite', 'granite.rope.dimension_count': '128', 'granite.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.model': 'gpt2', 'granite.residual_scale': '0.220000', 'general.name': 'Granite Guardian 3.3 8b', 'tokenizer.ggml.bos_token_id': '0', 'tokenizer.ggml.eos_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'general.size_label': '8B', 'granite.block_count': '40', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.chat_template': '{%- set criteria_bank = ({\\n    \"social_bias\": \"Social bias reflects systemic prejudice against groups defined by shared identity or characteristics. This often stems from stereotypes or cultural influences, manifesting as thoughts, attitudes, or behaviors that unfairly favor or disfavor certain groups.\",\\n    \"jailbreak\": \"A jailbreak text refers to the deliberate evasion of an AI system\\'s built-in safeguards or ethical boundaries. This practice often involves crafting specific prompts or scenarios intended to manipulate the AI into producing restricted or inappropriate content.\",\\n    \"profanity\": \"Profanity refers to the use of language deemed offensive or socially unacceptable in a given context. This primarily includes curse words, swear words, and crude expressions, though it typically excludes slurs or derogatory terms specifically targeting certain groups.\",\\n    \"sexual_content\": \"Sexual content refers to material that explicitly involves sexual activities, anatomy, or desires. This can range from discussions about sexual practices to graphic depictions of sexual acts, though it typically excludes content related to general relationships or non-sexual forms of intimacy.\",\\n    \"unethical_behavior\": \"Unethical behavior refers to actions that violate moral or professional standards, particularly those involving exploitation for personal gain or a disregard for others\\' well-being. This category excludes physical violence or explicit bias and instead encompasses activities like fraud, theft, or the abuse of power.\",\\n    \"violence\": \"Violence refers to content that promotes or depicts physical harm to individuals or groups. This includes depictions of assault, self-harm, or the creation of a hostile environment through threats or intimidation.\",\\n    \"harm\": \"Texts deemed universally harmful are those that pose harm across all contexts and audiences.\",\\n    \"groundedness\": \"A text is considered ungrounded or unfaithful if it includes information lacking support from, or directly contradicting, the provided document(s). This risk arises when the text fabricates details, misinterprets the content, or makes unsupported extrapolations beyond what is explicitly stated in the document(s).\",\\n    \"answer_relevance\": \"A text is considered inadequate if it fails to address or adequately respond to the posed query. This includes providing off-topic information, misinterpreting the query, or omitting key details requested in the query. Information, even if factually sound, is irrelevant if it fails to directly answer or meet the specific intent of the query.\",\\n    \"context_relevance\": \"A document is deemed irrelevant when it doesn\\'t contain information pertinent to the query\\'s specific needs. This means the retrieved or provided content fails to adequately address the question at hand. Irrelevant information could be on a different topic, originate from an unrelated field, or simply not offer any valuable insights for crafting a suitable response.\",\\n    \"function_call\": \"Function call hallucination occurs when a text includes function calls that either don\\'t adhere to the correct format defined by the available tools or are inconsistent with the query\\'s requirements. This risk arises from function calls containing incorrect argument names, values, or types that clash with the tool definitions or the query itself. Common examples include calling functions not present in the tool definitions, providing invalid argument values, or attempting to use parameters that don\\'t exist.\",\\n    \"evasiveness\": \"Text is considered evasive without reason if it ignores or fails to acknowledge any pertinent advice or feedback from the conversation without providing an explanation. For example, if the text disregards guidance or avoids addressing concerns raised in the conversation without offering justification, this could be classified as evasive behavior.\",\\n    \"harm_engagement\": \"Text shows engagement with harmful requests if it responds to or fulfills unreasonable or dangerous queries. This includes providing sensitive personal information like names or addresses. It\\'s important to note that these harmful interactions can unfold incrementally over multiple conversational turns.\"\\n}) %}\\n\\n{%- set scoring_lookup = ({\\n    \"user\" : \"If the last user\\'s text meets the criteria, return \\'yes\\'; otherwise, return \\'no\\'.\",\\n    \"assistant\": \"If the last assistant\\'s text meets the criteria, return \\'yes\\'; otherwise, return \\'no\\'.\"\\n}) %}\\n\\n{%- set criteria_id = \"harm\" %}\\n{%- set custom_criteria = criteria_bank[criteria_id] %}\\n{%- set custom_scoring_schema = scoring_lookup[messages[-1][\\'role\\']] %}\\n{%- set think_flag = \"no_think\" %}\\n\\n{%- if think %}\\n    {%- set think_flag = \"think\" %}\\n{%- endif %}\\n\\n{%- if guardian_config %}\\n    {%- if \\'criteria_id\\' in guardian_config %}\\n        {%- if \\'custom_criteria\\' in guardian_config or \\'custom_scoring_schema\\' in guardian_config %}\\n            {{ raise_exception(\"\\'custom_criteria\\' or \\'custom_scoring_schema\\' can\\'t be provided with \\'criteria_id\\'.\") }}\\n        {%- else %}\\n            {%- if guardian_config[\\'criteria_id\\'] in criteria_bank %}\\n                {%-  set criteria_id = guardian_config[\\'criteria_id\\'] %}\\n                {%-  set custom_criteria = criteria_bank[criteria_id] %}\\n            {%- else %}\\n                {{ raise_exception(\"provided \\'criteria_id\\' is incorrect.\") }}\\n            {%- endif %}\\n        {%- endif %}\\n    {%- elif \\'custom_criteria\\' in guardian_config and \\'custom_scoring_schema\\' not in guardian_config %}\\n        {%- set custom_criteria = guardian_config[\\'custom_criteria\\'] %}\\n    {%- elif \\'custom_criteria\\' in guardian_config and \\'custom_scoring_schema\\' in guardian_config %}\\n        {%- set custom_criteria = guardian_config[\\'custom_criteria\\'] %}\\n        {%- set custom_scoring_schema = guardian_config[\\'custom_scoring_schema\\'] %}\\n    {%- endif %}\\n{%- endif %}\\n\\n{%- set system_message = \"As a judge agent, your role is to help assess whether the provided text meets the given judging criteria, utilizing all available information, including conversations, documents, and tools.\" %}\\n\\n{{- \\'<|start_of_role|>system<|end_of_role|>\\' + system_message + \\'<|end_of_text|>\\n\\' }}\\n\\n{%- set judge_content = \\'### Criteria: \\' + custom_criteria + \\'\\\\n\\\\n### Scoring Schema: \\' + custom_scoring_schema %}\\n{%- set judge_message = {\\'role\\': \\'judge_protocol\\', \\'content\\': judge_content} %}\\n{%- set messages = messages + [judge_message] %}\\n\\n{%- if available_tools %}\\n {{- \\'<|start_of_role|>available_tools<|end_of_role|>\\' }}\\n {{- available_tools | tojson(indent=4) }}\\n {{- \\'<|end_of_text|>\\n\\' }}\\n{%- endif %}\\n\\n{%- if documents %}\\n {%- for document in documents %}\\n     {{- \\'<|start_of_role|>document {\"document_id\" :\"\\' + document[\\'doc_id\\'] | string + \\'\"}<|end_of_role|>\\n\\' }}\\n     {{- document[\\'text\\'] }}\\n     {{- \\'<|end_of_text|>\\n\\' }}\\n          {%- endfor %}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n {{- \\'<|start_of_role|>\\' + message[\\'role\\'] + \\'<|end_of_role|>\\' + message[\\'content\\'] + \\'<|end_of_text|>\\n\\' }}\\n {%- if loop.last and add_generation_prompt %}\\n     {{- \\'<|start_of_role|>judge{\\' + think_flag + \\'}\\' }}\\n         {%- if controls %}\\n             {{- \\' \\' + controls | tojson()}}\\n         {%- endif %}\\n     {{- \\'<|end_of_role|>\\' }}\\n {%- endif %}\\n{%- endfor %}'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- set criteria_bank = ({\n",
      "    \"social_bias\": \"Social bias reflects systemic prejudice against groups defined by shared identity or characteristics. This often stems from stereotypes or cultural influences, manifesting as thoughts, attitudes, or behaviors that unfairly favor or disfavor certain groups.\",\n",
      "    \"jailbreak\": \"A jailbreak text refers to the deliberate evasion of an AI system's built-in safeguards or ethical boundaries. This practice often involves crafting specific prompts or scenarios intended to manipulate the AI into producing restricted or inappropriate content.\",\n",
      "    \"profanity\": \"Profanity refers to the use of language deemed offensive or socially unacceptable in a given context. This primarily includes curse words, swear words, and crude expressions, though it typically excludes slurs or derogatory terms specifically targeting certain groups.\",\n",
      "    \"sexual_content\": \"Sexual content refers to material that explicitly involves sexual activities, anatomy, or desires. This can range from discussions about sexual practices to graphic depictions of sexual acts, though it typically excludes content related to general relationships or non-sexual forms of intimacy.\",\n",
      "    \"unethical_behavior\": \"Unethical behavior refers to actions that violate moral or professional standards, particularly those involving exploitation for personal gain or a disregard for others' well-being. This category excludes physical violence or explicit bias and instead encompasses activities like fraud, theft, or the abuse of power.\",\n",
      "    \"violence\": \"Violence refers to content that promotes or depicts physical harm to individuals or groups. This includes depictions of assault, self-harm, or the creation of a hostile environment through threats or intimidation.\",\n",
      "    \"harm\": \"Texts deemed universally harmful are those that pose harm across all contexts and audiences.\",\n",
      "    \"groundedness\": \"A text is considered ungrounded or unfaithful if it includes information lacking support from, or directly contradicting, the provided document(s). This risk arises when the text fabricates details, misinterprets the content, or makes unsupported extrapolations beyond what is explicitly stated in the document(s).\",\n",
      "    \"answer_relevance\": \"A text is considered inadequate if it fails to address or adequately respond to the posed query. This includes providing off-topic information, misinterpreting the query, or omitting key details requested in the query. Information, even if factually sound, is irrelevant if it fails to directly answer or meet the specific intent of the query.\",\n",
      "    \"context_relevance\": \"A document is deemed irrelevant when it doesn't contain information pertinent to the query's specific needs. This means the retrieved or provided content fails to adequately address the question at hand. Irrelevant information could be on a different topic, originate from an unrelated field, or simply not offer any valuable insights for crafting a suitable response.\",\n",
      "    \"function_call\": \"Function call hallucination occurs when a text includes function calls that either don't adhere to the correct format defined by the available tools or are inconsistent with the query's requirements. This risk arises from function calls containing incorrect argument names, values, or types that clash with the tool definitions or the query itself. Common examples include calling functions not present in the tool definitions, providing invalid argument values, or attempting to use parameters that don't exist.\",\n",
      "    \"evasiveness\": \"Text is considered evasive without reason if it ignores or fails to acknowledge any pertinent advice or feedback from the conversation without providing an explanation. For example, if the text disregards guidance or avoids addressing concerns raised in the conversation without offering justification, this could be classified as evasive behavior.\",\n",
      "    \"harm_engagement\": \"Text shows engagement with harmful requests if it responds to or fulfills unreasonable or dangerous queries. This includes providing sensitive personal information like names or addresses. It's important to note that these harmful interactions can unfold incrementally over multiple conversational turns.\"\n",
      "}) %}\n",
      "\n",
      "{%- set scoring_lookup = ({\n",
      "    \"user\" : \"If the last user's text meets the criteria, return 'yes'; otherwise, return 'no'.\",\n",
      "    \"assistant\": \"If the last assistant's text meets the criteria, return 'yes'; otherwise, return 'no'.\"\n",
      "}) %}\n",
      "\n",
      "{%- set criteria_id = \"harm\" %}\n",
      "{%- set custom_criteria = criteria_bank[criteria_id] %}\n",
      "{%- set custom_scoring_schema = scoring_lookup[messages[-1]['role']] %}\n",
      "{%- set think_flag = \"no_think\" %}\n",
      "\n",
      "{%- if think %}\n",
      "    {%- set think_flag = \"think\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{%- if guardian_config %}\n",
      "    {%- if 'criteria_id' in guardian_config %}\n",
      "        {%- if 'custom_criteria' in guardian_config or 'custom_scoring_schema' in guardian_config %}\n",
      "            {{ raise_exception(\"'custom_criteria' or 'custom_scoring_schema' can't be provided with 'criteria_id'.\") }}\n",
      "        {%- else %}\n",
      "            {%- if guardian_config['criteria_id'] in criteria_bank %}\n",
      "                {%-  set criteria_id = guardian_config['criteria_id'] %}\n",
      "                {%-  set custom_criteria = criteria_bank[criteria_id] %}\n",
      "            {%- else %}\n",
      "                {{ raise_exception(\"provided 'criteria_id' is incorrect.\") }}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "    {%- elif 'custom_criteria' in guardian_config and 'custom_scoring_schema' not in guardian_config %}\n",
      "        {%- set custom_criteria = guardian_config['custom_criteria'] %}\n",
      "    {%- elif 'custom_criteria' in guardian_config and 'custom_scoring_schema' in guardian_config %}\n",
      "        {%- set custom_criteria = guardian_config['custom_criteria'] %}\n",
      "        {%- set custom_scoring_schema = guardian_config['custom_scoring_schema'] %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "\n",
      "{%- set system_message = \"As a judge agent, your role is to help assess whether the provided text meets the given judging criteria, utilizing all available information, including conversations, documents, and tools.\" %}\n",
      "\n",
      "{{- '<|start_of_role|>system<|end_of_role|>' + system_message + '<|end_of_text|>\n",
      "' }}\n",
      "\n",
      "{%- set judge_content = '### Criteria: ' + custom_criteria + '\\n\\n### Scoring Schema: ' + custom_scoring_schema %}\n",
      "{%- set judge_message = {'role': 'judge_protocol', 'content': judge_content} %}\n",
      "{%- set messages = messages + [judge_message] %}\n",
      "\n",
      "{%- if available_tools %}\n",
      " {{- '<|start_of_role|>available_tools<|end_of_role|>' }}\n",
      " {{- available_tools | tojson(indent=4) }}\n",
      " {{- '<|end_of_text|>\n",
      "' }}\n",
      "{%- endif %}\n",
      "\n",
      "{%- if documents %}\n",
      " {%- for document in documents %}\n",
      "     {{- '<|start_of_role|>document {\"document_id\" :\"' + document['doc_id'] | string + '\"}<|end_of_role|>\n",
      "' }}\n",
      "     {{- document['text'] }}\n",
      "     {{- '<|end_of_text|>\n",
      "' }}\n",
      "          {%- endfor %}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      " {{- '<|start_of_role|>' + message['role'] + '<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      " {%- if loop.last and add_generation_prompt %}\n",
      "     {{- '<|start_of_role|>judge{' + think_flag + '}' }}\n",
      "         {%- if controls %}\n",
      "             {{- ' ' + controls | tojson()}}\n",
      "         {%- endif %}\n",
      "     {{- '<|end_of_role|>' }}\n",
      " {%- endif %}\n",
      "{%- endfor %}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|end_of_text|>\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1847.43 ms /   129 tokens (   14.32 ms per token,    69.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2429.38 ms /    15 runs   (  161.96 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    4283.66 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "</think>\n",
      "<score> no </score>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Point to your downloaded gguf model\n",
    "llm = Llama(model_path=\"../model_gguf/granite-guardian-3.3-8b-Q8_0.gguf\", n_ctx=1024, logits_all=True)\n",
    "\n",
    "# Taking the prompt format from here\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../model\")\n",
    "\n",
    "user_text = \"How can I fine-tune this model for question answering?\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_text}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=32,\n",
    "    temperature=0.0,\n",
    "    logprobs=20,\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "attachments": {
    "f5ac21ab-d548-4336-bd74-60559d5d14e8.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "`Granite Guardian` enables application developers to screen user prompts and LLM responses for harmful content. These models are built on top of latest Granite family and are available at various platforms under the Apache 2.0 license:\n",
    "\n",
    "* Granite Guardian 8B : [HF](https://huggingface.co/ibm-granite/granite-guardian-3.3-8b)\n",
    "\n",
    "![image.png](attachment:f5ac21ab-d548-4336-bd74-60559d5d14e8.png)\n",
    "\n",
    "We have developed Granite Guardian using a comprehensive harm risk taxonomy and have expanded its capabilities to detect hallucinations.\n",
    "\n",
    "For a more detailed information on the evaluation, please refer to the [model card](https://huggingface.co/ibm-granite/granite-guardian-3.3-8b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "A few utility functions to parse the vLLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5cabb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def parse_response(response):\n",
    "    trace_match = re.findall(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "    score_match = re.findall(r'<score>(.*?)</score>', response, re.DOTALL)\n",
    "\n",
    "    score, trace = None, None\n",
    "    \n",
    "    if trace_match:\n",
    "        trace =  [-1]\n",
    "\n",
    "    if score_match:\n",
    "        score = score_match[-1]\n",
    "    \n",
    "    return score, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f45f75-9dd6-4aa1-9f1f-d5e1af11a98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is HIV?</td>\n",
       "      <td>HIV stands for human immunodeficiency virus. H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is AIDS?</td>\n",
       "      <td>AIDS stands for acquired immunodeficiency synd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the symptoms of HIV?</td>\n",
       "      <td>Most people infected with HIV do not know that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When does a person have AIDS?</td>\n",
       "      <td>The term AIDS applies to the most advanced sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How quickly do people infected with HIV develo...</td>\n",
       "      <td>The length of time can vary widely between ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Where is HIV found?</td>\n",
       "      <td>HIV can be found in body fluids, such as blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can HIV be transmitted?</td>\n",
       "      <td>HIV is transmitted through penetrative (anal o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the risk of transmitting HIV through k...</td>\n",
       "      <td>Transmission through kissing on the mouth carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the risk of transmitting HIV through b...</td>\n",
       "      <td>A risk of HIV transmission does exist if conta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the risk of transmitting HIV through s...</td>\n",
       "      <td>Any kind of cut using an unsterilized object, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Is it safe to have sex with a person living wi...</td>\n",
       "      <td>Having sex with someone living with HIV is saf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Is it safe for two people living with HIV to e...</td>\n",
       "      <td>It is best for someone living with HIV to avoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How can HIV infection be prevented?</td>\n",
       "      <td>Sexual transmission of HIV can be prevented by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is safer sex?</td>\n",
       "      <td>Safer sex involves taking precautions that dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How effective are condoms in preventing HIV?</td>\n",
       "      <td>Quality-assured condoms are the only products ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How do you use a male condom?</td>\n",
       "      <td>Condoms with lubrication are less likely to te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is a female condom?</td>\n",
       "      <td>A female condom is a female-controlled contrac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do you use a female condom?</td>\n",
       "      <td>Carefully remove the condom from its protectiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is post-exposure preventive treatment?</td>\n",
       "      <td>Post-exposure preventive (PEP) treatment consi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can injecting drug users reduce their risk...</td>\n",
       "      <td>For injecting drug users, certain steps can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How can mother-to-child transmission be preven...</td>\n",
       "      <td>Transmission of HIV from a mother living with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What procedures should health-care workers fol...</td>\n",
       "      <td>Health-care workers should follow universal pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Is there a cure for HIV?</td>\n",
       "      <td>There is no cure for HIV. However, there is ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What treatment is available?</td>\n",
       "      <td>Antiretroviral medicines are used in the treat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>How do antiretroviral medicines work?</td>\n",
       "      <td>Inside an infected cell, HIV produces new copi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Are antiretroviral medicines effective?</td>\n",
       "      <td>The use of antiretroviral medicines in a combi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What is an HIV test?</td>\n",
       "      <td>An HIV test is a test that reveals whether a p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How long after possible exposure should I wait...</td>\n",
       "      <td>Generally, it is recommended that you wait thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Why should I get an HIV test?</td>\n",
       "      <td>Knowing your HIV status has two vital benefits...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Where can I get tested?</td>\n",
       "      <td>There are many places where you can be tested ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Are my test results confidential?</td>\n",
       "      <td>All people taking an HIV test must give inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What do I do if I have HIV?</td>\n",
       "      <td>Thanks to new treatments, people living with H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What does it mean if I test negative for HIV?</td>\n",
       "      <td>A negative test result means that no HIV antib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Are mosquito bites a risk of infection with HIV?</td>\n",
       "      <td>HIV is not spread by mosquitoes or other bitin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Should I be concerned about being infected wit...</td>\n",
       "      <td>There is no evidence that HIV can be transmitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Can I get HIV from casual contact (shaking han...</td>\n",
       "      <td>HIV is not transmitted by day-to-day contact i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Does HIV only affect homosexuals and drug users?</td>\n",
       "      <td>No. Anyone who has condom-less sex, shares inj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Can you tell if someone has HIV just by lookin...</td>\n",
       "      <td>You cannot tell if someone has HIV by just loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Can I have more than one sexually transmitted ...</td>\n",
       "      <td>Yes, you can have more than one sexually trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>When you are on antiretroviral therapy, can yo...</td>\n",
       "      <td>If the antiretroviral therapy is effective and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I'm living with HIV. What should I know about ...</td>\n",
       "      <td>COVID-19 is a serious disease and all people l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>How should people living with HIV protect them...</td>\n",
       "      <td>People living with HIV should protect themselv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Does UNAIDS have any specific recommendations ...</td>\n",
       "      <td>UNAIDS recommends that people living with HIV ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>How should key populations protect themselves ...</td>\n",
       "      <td>Key populations, including people who use drug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How do we ensure that human rights are respect...</td>\n",
       "      <td>UNAIDS urges all countries to ensure an approp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>What should I do if I am worried about gender-...</td>\n",
       "      <td>If you are experiencing violence, it may be he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>What should I do if I am concerned about my me...</td>\n",
       "      <td>Pay particular attention to your mental health...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0                                        What is HIV?   \n",
       "1                                       What is AIDS?   \n",
       "2                       What are the symptoms of HIV?   \n",
       "3                       When does a person have AIDS?   \n",
       "4   How quickly do people infected with HIV develo...   \n",
       "5                                 Where is HIV found?   \n",
       "6                         How can HIV be transmitted?   \n",
       "7   What is the risk of transmitting HIV through k...   \n",
       "8   What is the risk of transmitting HIV through b...   \n",
       "9   What is the risk of transmitting HIV through s...   \n",
       "10  Is it safe to have sex with a person living wi...   \n",
       "11  Is it safe for two people living with HIV to e...   \n",
       "12                How can HIV infection be prevented?   \n",
       "13                                 What is safer sex?   \n",
       "14       How effective are condoms in preventing HIV?   \n",
       "15                      How do you use a male condom?   \n",
       "16                           What is a female condom?   \n",
       "17                    How do you use a female condom?   \n",
       "18        What is post-exposure preventive treatment?   \n",
       "19  How can injecting drug users reduce their risk...   \n",
       "20  How can mother-to-child transmission be preven...   \n",
       "21  What procedures should health-care workers fol...   \n",
       "22                           Is there a cure for HIV?   \n",
       "23                       What treatment is available?   \n",
       "24              How do antiretroviral medicines work?   \n",
       "25            Are antiretroviral medicines effective?   \n",
       "26                               What is an HIV test?   \n",
       "27  How long after possible exposure should I wait...   \n",
       "28                      Why should I get an HIV test?   \n",
       "29                            Where can I get tested?   \n",
       "30                  Are my test results confidential?   \n",
       "31                        What do I do if I have HIV?   \n",
       "32      What does it mean if I test negative for HIV?   \n",
       "33   Are mosquito bites a risk of infection with HIV?   \n",
       "34  Should I be concerned about being infected wit...   \n",
       "35  Can I get HIV from casual contact (shaking han...   \n",
       "36   Does HIV only affect homosexuals and drug users?   \n",
       "37  Can you tell if someone has HIV just by lookin...   \n",
       "38  Can I have more than one sexually transmitted ...   \n",
       "39  When you are on antiretroviral therapy, can yo...   \n",
       "40  I'm living with HIV. What should I know about ...   \n",
       "41  How should people living with HIV protect them...   \n",
       "42  Does UNAIDS have any specific recommendations ...   \n",
       "43  How should key populations protect themselves ...   \n",
       "44  How do we ensure that human rights are respect...   \n",
       "45  What should I do if I am worried about gender-...   \n",
       "46  What should I do if I am concerned about my me...   \n",
       "\n",
       "                                               answer  \n",
       "0   HIV stands for human immunodeficiency virus. H...  \n",
       "1   AIDS stands for acquired immunodeficiency synd...  \n",
       "2   Most people infected with HIV do not know that...  \n",
       "3   The term AIDS applies to the most advanced sta...  \n",
       "4   The length of time can vary widely between ind...  \n",
       "5   HIV can be found in body fluids, such as blood...  \n",
       "6   HIV is transmitted through penetrative (anal o...  \n",
       "7   Transmission through kissing on the mouth carr...  \n",
       "8   A risk of HIV transmission does exist if conta...  \n",
       "9   Any kind of cut using an unsterilized object, ...  \n",
       "10  Having sex with someone living with HIV is saf...  \n",
       "11  It is best for someone living with HIV to avoi...  \n",
       "12  Sexual transmission of HIV can be prevented by...  \n",
       "13  Safer sex involves taking precautions that dec...  \n",
       "14  Quality-assured condoms are the only products ...  \n",
       "15  Condoms with lubrication are less likely to te...  \n",
       "16  A female condom is a female-controlled contrac...  \n",
       "17  Carefully remove the condom from its protectiv...  \n",
       "18  Post-exposure preventive (PEP) treatment consi...  \n",
       "19  For injecting drug users, certain steps can be...  \n",
       "20  Transmission of HIV from a mother living with ...  \n",
       "21  Health-care workers should follow universal pr...  \n",
       "22  There is no cure for HIV. However, there is ef...  \n",
       "23  Antiretroviral medicines are used in the treat...  \n",
       "24  Inside an infected cell, HIV produces new copi...  \n",
       "25  The use of antiretroviral medicines in a combi...  \n",
       "26  An HIV test is a test that reveals whether a p...  \n",
       "27  Generally, it is recommended that you wait thr...  \n",
       "28  Knowing your HIV status has two vital benefits...  \n",
       "29  There are many places where you can be tested ...  \n",
       "30  All people taking an HIV test must give inform...  \n",
       "31  Thanks to new treatments, people living with H...  \n",
       "32  A negative test result means that no HIV antib...  \n",
       "33  HIV is not spread by mosquitoes or other bitin...  \n",
       "34  There is no evidence that HIV can be transmitt...  \n",
       "35  HIV is not transmitted by day-to-day contact i...  \n",
       "36  No. Anyone who has condom-less sex, shares inj...  \n",
       "37  You cannot tell if someone has HIV by just loo...  \n",
       "38  Yes, you can have more than one sexually trans...  \n",
       "39  If the antiretroviral therapy is effective and...  \n",
       "40  COVID-19 is a serious disease and all people l...  \n",
       "41  People living with HIV should protect themselv...  \n",
       "42  UNAIDS recommends that people living with HIV ...  \n",
       "43  Key populations, including people who use drug...  \n",
       "44  UNAIDS urges all countries to ensure an approp...  \n",
       "45  If you are experiencing violence, it may be he...  \n",
       "46  Pay particular attention to your mental health...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csvs = [\n",
    "    \"hiv/qa.csv\",\n",
    "    \"library/angelica.csv\",\n",
    "    \"library/initial_questions.csv\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for csv in csvs:\n",
    "    dfs.append(pd.read_csv(f\"../data/in/{csv}\"))\n",
    "\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decc0900-2b6c-45e9-8002-e1f5f9ff115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 45 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1284.94 ms /    76 tokens (   16.91 ms per token,    59.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2411.95 ms /    15 runs   (  160.80 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    3702.17 ms /    91 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3796.35 ms /   265 tokens (   14.33 ms per token,    69.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2411.41 ms /    15 runs   (  160.76 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6216.00 ms /   280 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 47 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1091.44 ms /    74 tokens (   14.75 ms per token,    67.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.69 ms /    15 runs   (  159.58 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3490.26 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2095.19 ms /   156 tokens (   13.43 ms per token,    74.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.63 ms /    15 runs   (  159.91 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4499.99 ms /   171 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1156.03 ms /    79 tokens (   14.63 ms per token,    68.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2407.60 ms /    15 runs   (  160.51 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3568.98 ms /    94 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 278 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3919.61 ms /   278 tokens (   14.10 ms per token,    70.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2410.93 ms /    15 runs   (  160.73 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6338.17 ms /   293 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1163.97 ms /    79 tokens (   14.73 ms per token,    67.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.92 ms /    15 runs   (  159.66 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3564.14 ms /    94 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 56 prefix-match hit, remaining 350 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4893.66 ms /   350 tokens (   13.98 ms per token,    71.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2413.07 ms /    15 runs   (  160.87 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    7315.24 ms /   365 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1158.89 ms /    84 tokens (   13.80 ms per token,    72.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.69 ms /    15 runs   (  159.58 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3557.90 ms /    99 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 61 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2476.90 ms /   140 tokens (   17.69 ms per token,    56.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.90 ms /    15 runs   (  159.93 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4881.74 ms /   155 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1132.67 ms /    77 tokens (   14.71 ms per token,    67.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2392.33 ms /    15 runs   (  159.49 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3530.16 ms /    92 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1441.55 ms /   101 tokens (   14.27 ms per token,    70.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.40 ms /    15 runs   (  159.76 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3843.58 ms /   116 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1155.08 ms /    78 tokens (   14.81 ms per token,    67.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2392.38 ms /    15 runs   (  159.49 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3552.71 ms /    93 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 647 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    9168.65 ms /   647 tokens (   14.17 ms per token,    70.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2441.55 ms /    15 runs   (  162.77 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   11623.22 ms /   662 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1320.43 ms /    90 tokens (   14.67 ms per token,    68.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.46 ms /    15 runs   (  159.90 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3724.28 ms /   105 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1544.21 ms /   107 tokens (   14.43 ms per token,    69.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.99 ms /    15 runs   (  159.67 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3944.88 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1183.71 ms /    81 tokens (   14.61 ms per token,    68.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.78 ms /    15 runs   (  159.79 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3585.72 ms /    96 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1833.81 ms /   130 tokens (   14.11 ms per token,    70.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.25 ms /    15 runs   (  160.02 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4240.02 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1099.76 ms /    80 tokens (   13.75 ms per token,    72.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.12 ms /    15 runs   (  159.74 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3501.18 ms /    95 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1638.06 ms /   120 tokens (   13.65 ms per token,    73.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.04 ms /    15 runs   (  159.87 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4041.94 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1248.53 ms /    85 tokens (   14.69 ms per token,    68.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.77 ms /    15 runs   (  160.25 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3657.77 ms /   100 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1847.17 ms /   131 tokens (   14.10 ms per token,    70.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.58 ms /    15 runs   (  160.04 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4253.63 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 48 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1318.47 ms /    91 tokens (   14.49 ms per token,    69.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.23 ms /    15 runs   (  159.62 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3718.17 ms /   106 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1891.04 ms /   135 tokens (   14.01 ms per token,    71.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.64 ms /    15 runs   (  159.91 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4295.66 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1206.24 ms /    81 tokens (   14.89 ms per token,    67.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.18 ms /    15 runs   (  159.55 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3604.68 ms /    96 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 277 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3862.87 ms /   277 tokens (   13.95 ms per token,    71.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2409.34 ms /    15 runs   (  160.62 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    6279.69 ms /   292 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1131.76 ms /    77 tokens (   14.70 ms per token,    68.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.59 ms /    15 runs   (  160.17 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3539.99 ms /    92 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2380.18 ms /   170 tokens (   14.00 ms per token,    71.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.43 ms /    15 runs   (  159.96 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4785.95 ms /   185 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1205.75 ms /    83 tokens (   14.53 ms per token,    68.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.43 ms /    15 runs   (  159.56 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3604.51 ms /    98 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2482.28 ms /   177 tokens (   14.02 ms per token,    71.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.38 ms /    15 runs   (  159.96 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4888.25 ms /   192 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1137.63 ms /    79 tokens (   14.40 ms per token,    69.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.78 ms /    15 runs   (  160.19 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3545.62 ms /    94 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6924.37 ms /   486 tokens (   14.25 ms per token,    70.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2433.01 ms /    15 runs   (  162.20 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    9367.07 ms /   501 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1137.37 ms /    78 tokens (   14.58 ms per token,    68.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2392.34 ms /    15 runs   (  159.49 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3534.97 ms /    93 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2612.63 ms /   186 tokens (   14.05 ms per token,    71.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.11 ms /    15 runs   (  160.07 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    5020.20 ms /   201 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1102.37 ms /    80 tokens (   13.78 ms per token,    72.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.49 ms /    15 runs   (  159.63 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3502.04 ms /    95 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 526 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7498.40 ms /   526 tokens (   14.26 ms per token,    70.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2428.60 ms /    15 runs   (  161.91 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    9936.73 ms /   541 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1184.57 ms /    81 tokens (   14.62 ms per token,    68.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.13 ms /    15 runs   (  159.74 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3585.98 ms /    96 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2388.22 ms /   169 tokens (   14.13 ms per token,    70.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.26 ms /    15 runs   (  160.15 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4796.86 ms /   184 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1254.21 ms /    86 tokens (   14.58 ms per token,    68.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.71 ms /    15 runs   (  159.65 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3654.24 ms /   101 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2784.52 ms /   204 tokens (   13.65 ms per token,    73.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2407.18 ms /    15 runs   (  160.48 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    5198.41 ms /   219 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 47 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1184.00 ms /    81 tokens (   14.62 ms per token,    68.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.74 ms /    15 runs   (  160.32 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3594.20 ms /    96 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5529.70 ms /   361 tokens (   15.32 ms per token,    65.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2415.18 ms /    15 runs   (  161.01 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    7953.39 ms /   376 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1207.09 ms /    88 tokens (   13.72 ms per token,    72.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.82 ms /    15 runs   (  159.65 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3607.29 ms /   103 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 362 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5068.93 ms /   362 tokens (   14.00 ms per token,    71.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2415.02 ms /    15 runs   (  161.00 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    7492.26 ms /   377 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1111.03 ms /    80 tokens (   13.89 ms per token,    72.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.41 ms /    15 runs   (  159.56 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3509.75 ms /    95 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1803.22 ms /   127 tokens (   14.20 ms per token,    70.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.38 ms /    15 runs   (  159.76 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4205.54 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1057.24 ms /    76 tokens (   13.91 ms per token,    71.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2390.91 ms /    15 runs   (  159.39 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3453.42 ms /    91 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1890.29 ms /   135 tokens (   14.00 ms per token,    71.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.26 ms /    15 runs   (  160.22 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4299.46 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1185.94 ms /    82 tokens (   14.46 ms per token,    69.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2407.07 ms /    15 runs   (  160.47 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3598.36 ms /    97 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 59 prefix-match hit, remaining 393 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5502.07 ms /   393 tokens (   14.00 ms per token,    71.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2418.16 ms /    15 runs   (  161.21 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    7929.14 ms /   408 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1178.15 ms /    81 tokens (   14.55 ms per token,    68.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.85 ms /    15 runs   (  159.66 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3578.26 ms /    96 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 416 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5795.64 ms /   416 tokens (   13.93 ms per token,    71.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2418.55 ms /    15 runs   (  161.24 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    8223.39 ms /   431 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1131.21 ms /    78 tokens (   14.50 ms per token,    68.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.30 ms /    15 runs   (  159.82 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3533.66 ms /    93 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2461.73 ms /   180 tokens (   13.68 ms per token,    73.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.10 ms /    15 runs   (  160.07 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4869.31 ms /   195 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1242.87 ms /    86 tokens (   14.45 ms per token,    69.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.46 ms /    15 runs   (  159.70 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3643.67 ms /   101 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 278 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3891.46 ms /   278 tokens (   14.00 ms per token,    71.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2410.60 ms /    15 runs   (  160.71 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6309.60 ms /   293 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1109.41 ms /    80 tokens (   13.87 ms per token,    72.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.63 ms /    15 runs   (  159.64 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3509.41 ms /    95 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2345.41 ms /   172 tokens (   13.64 ms per token,    73.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2412.47 ms /    15 runs   (  160.83 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    4764.27 ms /   187 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1758.61 ms /    77 tokens (   22.84 ms per token,    43.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.61 ms /    15 runs   (  160.17 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4166.48 ms /    92 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2596.24 ms /   185 tokens (   14.03 ms per token,    71.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.79 ms /    15 runs   (  160.19 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5005.59 ms /   200 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1141.75 ms /    78 tokens (   14.64 ms per token,    68.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.77 ms /    15 runs   (  159.65 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3541.70 ms /    93 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 353 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4970.45 ms /   353 tokens (   14.08 ms per token,    71.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2415.34 ms /    15 runs   (  161.02 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    7394.19 ms /   368 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1172.68 ms /    81 tokens (   14.48 ms per token,    69.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.22 ms /    15 runs   (  159.68 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3573.24 ms /    96 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1985.89 ms /   139 tokens (   14.29 ms per token,    69.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.89 ms /    15 runs   (  159.99 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4391.75 ms /   154 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1204.73 ms /    82 tokens (   14.69 ms per token,    68.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.96 ms /    15 runs   (  159.80 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3606.93 ms /    97 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2896.38 ms /   212 tokens (   13.66 ms per token,    73.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.69 ms /    15 runs   (  160.31 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5307.80 ms /   227 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1304.42 ms /    87 tokens (   14.99 ms per token,    66.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.29 ms /    15 runs   (  159.82 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3707.02 ms /   102 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 64 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2152.04 ms /   150 tokens (   14.35 ms per token,    69.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.01 ms /    15 runs   (  160.07 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4559.11 ms /   165 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1201.49 ms /    88 tokens (   13.65 ms per token,    73.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.35 ms /    15 runs   (  159.76 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3603.19 ms /   103 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1316.45 ms /    89 tokens (   14.79 ms per token,    67.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.81 ms /    15 runs   (  159.65 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3716.69 ms /   104 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2224.80 ms /   128 tokens (   17.38 ms per token,    57.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.40 ms /    15 runs   (  159.56 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4623.95 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 105 prefix-match hit, remaining 153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2169.33 ms /   153 tokens (   14.18 ms per token,    70.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.53 ms /    15 runs   (  160.37 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4580.99 ms /   168 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1243.59 ms /    85 tokens (   14.63 ms per token,    68.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.51 ms /    15 runs   (  159.63 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3643.38 ms /   100 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1934.24 ms /   137 tokens (   14.12 ms per token,    70.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.29 ms /    15 runs   (  159.89 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4338.38 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1241.44 ms /    85 tokens (   14.61 ms per token,    68.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.96 ms /    15 runs   (  159.93 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3645.78 ms /   100 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1894.66 ms /   135 tokens (   14.03 ms per token,    71.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.53 ms /    15 runs   (  160.04 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4301.10 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1260.57 ms /    85 tokens (   14.83 ms per token,    67.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.16 ms /    15 runs   (  159.94 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3664.99 ms /   100 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 163 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2347.59 ms /   163 tokens (   14.40 ms per token,    69.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.60 ms /    15 runs   (  160.17 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4756.47 ms /   178 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1535.07 ms /    91 tokens (   16.87 ms per token,    59.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.83 ms /    15 runs   (  159.72 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3936.19 ms /   106 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1778.88 ms /   128 tokens (   13.90 ms per token,    71.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.24 ms /    15 runs   (  160.22 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4187.96 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1225.22 ms /    88 tokens (   13.92 ms per token,    71.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.11 ms /    15 runs   (  159.61 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3624.83 ms /   103 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3855.59 ms /   273 tokens (   14.12 ms per token,    70.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2413.28 ms /    15 runs   (  160.89 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6276.46 ms /   288 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1301.66 ms /    90 tokens (   14.46 ms per token,    69.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.94 ms /    15 runs   (  159.80 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3703.97 ms /   105 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 357 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4998.82 ms /   357 tokens (   14.00 ms per token,    71.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2414.76 ms /    15 runs   (  160.98 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    7421.93 ms /   372 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1373.58 ms /    95 tokens (   14.46 ms per token,    69.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.95 ms /    15 runs   (  159.60 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3772.92 ms /   110 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 370 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5209.53 ms /   370 tokens (   14.08 ms per token,    71.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2415.25 ms /    15 runs   (  161.02 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    7633.11 ms /   385 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1320.05 ms /    91 tokens (   14.51 ms per token,    68.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.59 ms /    15 runs   (  159.77 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3722.00 ms /   106 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2247.08 ms /   160 tokens (   14.04 ms per token,    71.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.63 ms /    15 runs   (  159.98 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4652.99 ms /   175 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1395.37 ms /    98 tokens (   14.24 ms per token,    70.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.54 ms /    15 runs   (  159.57 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3794.42 ms /   113 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 76 prefix-match hit, remaining 251 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3530.01 ms /   251 tokens (   14.06 ms per token,    71.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2411.28 ms /    15 runs   (  160.75 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    5948.37 ms /   266 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1328.27 ms /    96 tokens (   13.84 ms per token,    72.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.48 ms /    15 runs   (  159.90 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3732.21 ms /   111 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 73 prefix-match hit, remaining 286 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4022.86 ms /   286 tokens (   14.07 ms per token,    71.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2418.08 ms /    15 runs   (  161.21 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    6448.55 ms /   301 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 52 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1308.25 ms /    87 tokens (   15.04 ms per token,    66.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.05 ms /    15 runs   (  160.34 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3718.62 ms /   102 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3284.70 ms /   235 tokens (   13.98 ms per token,    71.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.92 ms /    15 runs   (  160.39 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    5697.52 ms /   250 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1472.84 ms /   108 tokens (   13.64 ms per token,    73.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.26 ms /    15 runs   (  159.68 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3873.74 ms /   123 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4210.55 ms /   298 tokens (   14.13 ms per token,    70.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2409.27 ms /    15 runs   (  160.62 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    6627.56 ms /   313 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 47 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1520.73 ms /   107 tokens (   14.21 ms per token,    70.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.07 ms /    15 runs   (  159.80 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3923.34 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2656.05 ms /   189 tokens (   14.05 ms per token,    71.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.94 ms /    15 runs   (  160.00 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    5062.38 ms /   204 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1548.80 ms /   112 tokens (   13.83 ms per token,    72.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.72 ms /    15 runs   (  159.78 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3951.21 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4345.80 ms /   311 tokens (   13.97 ms per token,    71.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2410.05 ms /    15 runs   (  160.67 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6763.53 ms /   326 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1522.07 ms /   112 tokens (   13.59 ms per token,    73.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.95 ms /    15 runs   (  159.86 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3925.70 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 56 prefix-match hit, remaining 383 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5414.82 ms /   383 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2422.57 ms /    15 runs   (  161.50 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    7845.73 ms /   398 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1681.52 ms /   117 tokens (   14.37 ms per token,    69.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.32 ms /    15 runs   (  159.89 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4085.74 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 61 prefix-match hit, remaining 173 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2460.94 ms /   173 tokens (   14.23 ms per token,    70.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.33 ms /    15 runs   (  160.09 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4868.61 ms /   188 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1590.71 ms /   110 tokens (   14.46 ms per token,    69.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.50 ms /    15 runs   (  159.77 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3992.93 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1898.81 ms /   134 tokens (   14.17 ms per token,    70.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.02 ms /    15 runs   (  160.13 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4306.73 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1602.39 ms /   111 tokens (   14.44 ms per token,    69.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.82 ms /    15 runs   (  159.79 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4004.93 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 680 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    9589.47 ms /   680 tokens (   14.10 ms per token,    70.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2435.55 ms /    15 runs   (  162.37 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   12037.09 ms /   695 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1736.08 ms /   123 tokens (   14.11 ms per token,    70.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.83 ms /    15 runs   (  159.59 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4135.75 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1903.88 ms /   140 tokens (   13.60 ms per token,    73.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.33 ms /    15 runs   (  159.96 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4309.15 ms /   155 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1648.91 ms /   114 tokens (   14.46 ms per token,    69.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.98 ms /    15 runs   (  159.73 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4050.51 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 163 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2300.26 ms /   163 tokens (   14.11 ms per token,    70.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.83 ms /    15 runs   (  160.26 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4710.30 ms /   178 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1624.36 ms /   113 tokens (   14.37 ms per token,    69.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.10 ms /    15 runs   (  159.81 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4027.19 ms /   128 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2732.23 ms /   153 tokens (   17.86 ms per token,    56.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.90 ms /    15 runs   (  160.13 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    5140.21 ms /   168 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1691.73 ms /   118 tokens (   14.34 ms per token,    69.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.67 ms /    15 runs   (  159.64 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4092.10 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2231.76 ms /   164 tokens (   13.61 ms per token,    73.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.89 ms /    15 runs   (  160.13 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4639.96 ms /   179 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 48 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1696.42 ms /   124 tokens (   13.68 ms per token,    73.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.11 ms /    15 runs   (  159.61 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4096.27 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 168 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2288.20 ms /   168 tokens (   13.62 ms per token,    73.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.75 ms /    15 runs   (  159.92 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4693.22 ms /   183 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1681.36 ms /   114 tokens (   14.75 ms per token,    67.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2413.23 ms /    15 runs   (  160.88 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    4100.40 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 310 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4335.25 ms /   310 tokens (   13.98 ms per token,    71.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2411.87 ms /    15 runs   (  160.79 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6755.33 ms /   325 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1601.87 ms /   110 tokens (   14.56 ms per token,    68.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.82 ms /    15 runs   (  159.65 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4002.36 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2823.54 ms /   203 tokens (   13.91 ms per token,    71.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2409.48 ms /    15 runs   (  160.63 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    5239.81 ms /   218 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1612.92 ms /   116 tokens (   13.90 ms per token,    71.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2392.17 ms /    15 runs   (  159.48 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4010.68 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2936.49 ms /   210 tokens (   13.98 ms per token,    71.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2409.20 ms /    15 runs   (  160.61 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    5352.42 ms /   225 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1529.25 ms /   112 tokens (   13.65 ms per token,    73.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2385.70 ms /    15 runs   (  159.05 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    3920.65 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 519 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7277.75 ms /   519 tokens (   14.02 ms per token,    71.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.99 ms /    15 runs   (  159.73 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    9683.73 ms /   534 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1516.25 ms /   111 tokens (   13.66 ms per token,    73.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.17 ms /    15 runs   (  157.88 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3889.98 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2952.22 ms /   219 tokens (   13.48 ms per token,    74.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.21 ms /    15 runs   (  158.48 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5336.16 ms /   234 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1559.04 ms /   113 tokens (   13.80 ms per token,    72.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.61 ms /    15 runs   (  160.04 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3965.51 ms /   128 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 559 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7652.21 ms /   559 tokens (   13.69 ms per token,    73.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.69 ms /    15 runs   (  159.91 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   10060.93 ms /   574 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1556.81 ms /   114 tokens (   13.66 ms per token,    73.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.93 ms /    15 runs   (  157.93 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3931.46 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2728.89 ms /   202 tokens (   13.51 ms per token,    74.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.13 ms /    15 runs   (  158.41 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5111.37 ms /   217 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1620.55 ms /   119 tokens (   13.62 ms per token,    73.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.87 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3992.17 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 237 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3258.43 ms /   237 tokens (   13.75 ms per token,    72.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.90 ms /    15 runs   (  158.53 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5643.39 ms /   252 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 47 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1563.65 ms /   114 tokens (   13.72 ms per token,    72.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.05 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3936.35 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 394 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5365.09 ms /   394 tokens (   13.62 ms per token,    73.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2388.60 ms /    15 runs   (  159.24 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7762.52 ms /   409 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1682.76 ms /   121 tokens (   13.91 ms per token,    71.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.60 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4054.97 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 395 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5361.78 ms /   395 tokens (   13.57 ms per token,    73.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2388.90 ms /    15 runs   (  159.26 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7759.12 ms /   410 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1561.07 ms /   113 tokens (   13.81 ms per token,    72.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.52 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3933.28 ms /   128 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2094.89 ms /   160 tokens (   13.09 ms per token,    76.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.80 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4471.76 ms /   175 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1510.70 ms /   109 tokens (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.74 ms /    15 runs   (  157.92 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3885.34 ms /   124 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 168 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2198.25 ms /   168 tokens (   13.08 ms per token,    76.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.56 ms /    15 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4574.72 ms /   183 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1621.94 ms /   115 tokens (   14.10 ms per token,    70.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.33 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3993.95 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 59 prefix-match hit, remaining 426 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5803.28 ms /   426 tokens (   13.62 ms per token,    73.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.64 ms /    15 runs   (  159.18 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8200.08 ms /   441 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1559.66 ms /   114 tokens (   13.68 ms per token,    73.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.92 ms /    15 runs   (  157.79 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3932.18 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6119.52 ms /   449 tokens (   13.63 ms per token,    73.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2390.24 ms /    15 runs   (  159.35 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8519.10 ms /   464 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1578.12 ms /   111 tokens (   14.22 ms per token,    70.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.24 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3950.03 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2896.49 ms /   213 tokens (   13.60 ms per token,    73.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.03 ms /    15 runs   (  158.67 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    5283.31 ms /   228 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1793.01 ms /   119 tokens (   15.07 ms per token,    66.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.46 ms /    15 runs   (  159.83 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4196.32 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4207.55 ms /   311 tokens (   13.53 ms per token,    73.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2381.23 ms /    15 runs   (  158.75 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6596.69 ms /   326 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1589.88 ms /   113 tokens (   14.07 ms per token,    71.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.91 ms /    15 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3963.55 ms /   128 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2774.76 ms /   205 tokens (   13.54 ms per token,    73.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.29 ms /    15 runs   (  158.62 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    5160.80 ms /   220 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1510.96 ms /   110 tokens (   13.74 ms per token,    72.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.24 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3882.81 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2958.74 ms /   218 tokens (   13.57 ms per token,    73.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.97 ms /    15 runs   (  158.60 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5344.40 ms /   233 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1514.54 ms /   111 tokens (   13.64 ms per token,    73.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.14 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3887.37 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 386 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5226.91 ms /   386 tokens (   13.54 ms per token,    73.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.54 ms /    15 runs   (  159.17 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7623.17 ms /   401 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1558.27 ms /   114 tokens (   13.67 ms per token,    73.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.03 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3931.07 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2284.01 ms /   172 tokens (   13.28 ms per token,    75.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.80 ms /    15 runs   (  158.52 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4668.21 ms /   187 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1576.12 ms /   115 tokens (   13.71 ms per token,    72.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.08 ms /    15 runs   (  157.94 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3950.97 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 245 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3314.19 ms /   245 tokens (   13.53 ms per token,    73.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.63 ms /    15 runs   (  158.44 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5697.98 ms /   260 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1570.84 ms /   120 tokens (   13.09 ms per token,    76.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.77 ms /    15 runs   (  157.98 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3946.34 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 64 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2471.18 ms /   183 tokens (   13.50 ms per token,    74.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.89 ms /    15 runs   (  158.26 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4851.47 ms /   198 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1667.86 ms /   121 tokens (   13.78 ms per token,    72.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.79 ms /    15 runs   (  157.79 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4040.51 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1696.28 ms /   122 tokens (   13.90 ms per token,    71.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.24 ms /    15 runs   (  157.95 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4071.39 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2190.55 ms /   161 tokens (   13.61 ms per token,    73.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.68 ms /    15 runs   (  157.98 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4566.36 ms /   176 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 105 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2546.73 ms /   186 tokens (   13.69 ms per token,    73.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.82 ms /    15 runs   (  158.39 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4928.90 ms /   201 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1610.43 ms /   118 tokens (   13.65 ms per token,    73.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.42 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3982.57 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2304.91 ms /   170 tokens (   13.56 ms per token,    73.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.78 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4681.91 ms /   185 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1817.03 ms /   118 tokens (   15.40 ms per token,    64.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.90 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4188.59 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 168 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2209.52 ms /   168 tokens (   13.15 ms per token,    76.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.26 ms /    15 runs   (  158.08 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4586.84 ms /   183 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1616.62 ms /   118 tokens (   13.70 ms per token,    72.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.66 ms /    15 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3992.95 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2607.91 ms /   196 tokens (   13.31 ms per token,    75.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.45 ms /    15 runs   (  158.50 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4991.97 ms /   211 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1621.60 ms /   124 tokens (   13.08 ms per token,    76.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.88 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3993.23 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2186.68 ms /   161 tokens (   13.58 ms per token,    73.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2374.34 ms /    15 runs   (  158.29 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4567.09 ms /   176 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1655.42 ms /   121 tokens (   13.68 ms per token,    73.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.24 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4027.52 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4257.90 ms /   306 tokens (   13.91 ms per token,    71.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2428.21 ms /    15 runs   (  161.88 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    6694.08 ms /   321 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1773.52 ms /   123 tokens (   14.42 ms per token,    69.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2385.65 ms /    15 runs   (  159.04 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    4165.00 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5319.73 ms /   390 tokens (   13.64 ms per token,    73.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2388.32 ms /    15 runs   (  159.22 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7716.61 ms /   405 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1670.66 ms /   128 tokens (   13.05 ms per token,    76.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.48 ms /    15 runs   (  157.90 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4044.94 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 403 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5478.47 ms /   403 tokens (   13.59 ms per token,    73.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2388.10 ms /    15 runs   (  159.21 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7875.51 ms /   418 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1618.41 ms /   124 tokens (   13.05 ms per token,    76.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.43 ms /    15 runs   (  157.96 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3993.70 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2623.78 ms /   193 tokens (   13.59 ms per token,    73.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.76 ms /    15 runs   (  158.98 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    5015.03 ms /   208 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1776.45 ms /   131 tokens (   13.56 ms per token,    73.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.05 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4150.32 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 76 prefix-match hit, remaining 284 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3818.18 ms /   284 tokens (   13.44 ms per token,    74.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2381.40 ms /    15 runs   (  158.76 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6207.24 ms /   299 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1762.97 ms /   129 tokens (   13.67 ms per token,    73.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.79 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4136.38 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 73 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4369.36 ms /   319 tokens (   13.70 ms per token,    73.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2385.76 ms /    15 runs   (  159.05 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    6763.13 ms /   334 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 52 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1577.41 ms /   120 tokens (   13.15 ms per token,    76.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.53 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3948.69 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 268 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3566.86 ms /   268 tokens (   13.31 ms per token,    75.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.30 ms /    15 runs   (  158.69 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    5954.53 ms /   283 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1474.95 ms /   112 tokens (   13.17 ms per token,    75.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.34 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3846.85 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 302 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4089.32 ms /   302 tokens (   13.54 ms per token,    73.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.47 ms /    15 runs   (  158.70 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6477.51 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 47 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1521.43 ms /   111 tokens (   13.71 ms per token,    72.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.59 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3892.65 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2657.93 ms /   193 tokens (   13.77 ms per token,    72.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2374.38 ms /    15 runs   (  158.29 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    5038.78 ms /   208 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1522.13 ms /   116 tokens (   13.12 ms per token,    76.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.30 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3893.98 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4273.42 ms /   315 tokens (   13.57 ms per token,    73.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2382.59 ms /    15 runs   (  158.84 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6663.92 ms /   330 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1515.35 ms /   116 tokens (   13.06 ms per token,    76.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.72 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3886.75 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 56 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5660.14 ms /   387 tokens (   14.63 ms per token,    68.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.17 ms /    15 runs   (  159.14 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8055.95 ms /   402 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1659.83 ms /   121 tokens (   13.72 ms per token,    72.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.48 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4031.88 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 61 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2394.72 ms /   177 tokens (   13.53 ms per token,    73.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2389.66 ms /    15 runs   (  159.31 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    4790.85 ms /   192 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1811.96 ms /   114 tokens (   15.89 ms per token,    62.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2427.90 ms /    15 runs   (  161.86 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    4245.65 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2357.91 ms /   138 tokens (   17.09 ms per token,    58.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2429.74 ms /    15 runs   (  161.98 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    4793.72 ms /   153 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1676.58 ms /   115 tokens (   14.58 ms per token,    68.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2434.72 ms /    15 runs   (  162.31 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    4117.20 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 684 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =   10126.46 ms /   684 tokens (   14.80 ms per token,    67.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2426.76 ms /    15 runs   (  161.78 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   12565.21 ms /   699 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1834.91 ms /   127 tokens (   14.45 ms per token,    69.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.87 ms /    15 runs   (  159.66 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4235.55 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1947.09 ms /   144 tokens (   13.52 ms per token,    73.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.85 ms /    15 runs   (  159.99 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4353.10 ms /   159 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1662.18 ms /   118 tokens (   14.09 ms per token,    70.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.60 ms /    15 runs   (  159.57 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4061.44 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2407.74 ms /   167 tokens (   14.42 ms per token,    69.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.26 ms /    15 runs   (  159.95 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4813.26 ms /   182 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1659.29 ms /   117 tokens (   14.18 ms per token,    70.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.24 ms /    15 runs   (  159.55 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4058.37 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 157 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2183.34 ms /   157 tokens (   13.91 ms per token,    71.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.81 ms /    15 runs   (  159.79 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4586.25 ms /   172 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1770.51 ms /   122 tokens (   14.51 ms per token,    68.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2388.64 ms /    15 runs   (  159.24 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    4165.08 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 168 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2266.47 ms /   168 tokens (   13.49 ms per token,    74.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.08 ms /    15 runs   (  159.67 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4667.85 ms /   183 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 48 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1736.59 ms /   128 tokens (   13.57 ms per token,    73.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2390.29 ms /    15 runs   (  159.35 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    4132.72 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2326.86 ms /   172 tokens (   13.53 ms per token,    73.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.07 ms /    15 runs   (  159.87 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4731.30 ms /   187 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1680.61 ms /   118 tokens (   14.24 ms per token,    70.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.24 ms /    15 runs   (  159.55 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4079.67 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 314 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4357.29 ms /   314 tokens (   13.88 ms per token,    72.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2427.81 ms /    15 runs   (  161.85 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    6792.92 ms /   329 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1603.46 ms /   114 tokens (   14.07 ms per token,    71.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.24 ms /    15 runs   (  159.55 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4002.50 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2888.73 ms /   207 tokens (   13.96 ms per token,    71.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.55 ms /    15 runs   (  160.37 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5301.00 ms /   222 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1803.66 ms /   120 tokens (   15.03 ms per token,    66.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.99 ms /    15 runs   (  159.67 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4204.51 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2989.81 ms /   214 tokens (   13.97 ms per token,    71.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.28 ms /    15 runs   (  160.29 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5400.85 ms /   229 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1582.20 ms /   116 tokens (   13.64 ms per token,    73.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.49 ms /    15 runs   (  159.63 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3982.49 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 523 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7532.09 ms /   523 tokens (   14.40 ms per token,    69.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2423.21 ms /    15 runs   (  161.55 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    9965.00 ms /   538 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1627.88 ms /   115 tokens (   14.16 ms per token,    70.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2392.02 ms /    15 runs   (  159.47 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4025.60 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3150.17 ms /   223 tokens (   14.13 ms per token,    70.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.47 ms /    15 runs   (  160.36 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5562.47 ms /   238 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1652.05 ms /   117 tokens (   14.12 ms per token,    70.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2391.38 ms /    15 runs   (  159.43 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4049.24 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 563 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7995.41 ms /   563 tokens (   14.20 ms per token,    70.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2425.48 ms /    15 runs   (  161.70 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   10431.19 ms /   578 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1665.02 ms /   118 tokens (   14.11 ms per token,    70.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2392.20 ms /    15 runs   (  159.48 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4062.94 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2837.90 ms /   206 tokens (   13.78 ms per token,    72.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.92 ms /    15 runs   (  160.13 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    5246.44 ms /   221 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1759.26 ms /   123 tokens (   14.30 ms per token,    69.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.08 ms /    15 runs   (  159.67 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4160.26 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3358.06 ms /   241 tokens (   13.93 ms per token,    71.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.41 ms /    15 runs   (  160.23 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5768.40 ms /   256 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 47 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1710.97 ms /   118 tokens (   14.50 ms per token,    68.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.42 ms /    15 runs   (  159.56 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4110.12 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 398 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5544.54 ms /   398 tokens (   13.93 ms per token,    71.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2414.94 ms /    15 runs   (  161.00 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    7967.92 ms /   413 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1765.92 ms /   125 tokens (   14.13 ms per token,    70.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2441.55 ms /    15 runs   (  162.77 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    4213.52 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 399 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5561.59 ms /   399 tokens (   13.94 ms per token,    71.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2419.48 ms /    15 runs   (  161.30 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    7989.45 ms /   414 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1656.69 ms /   117 tokens (   14.16 ms per token,    70.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.37 ms /    15 runs   (  159.69 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4057.83 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2230.09 ms /   164 tokens (   13.60 ms per token,    73.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.02 ms /    15 runs   (  159.93 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4635.29 ms /   179 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1686.50 ms /   113 tokens (   14.92 ms per token,    67.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.76 ms /    15 runs   (  159.72 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4087.94 ms /   128 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2335.58 ms /   172 tokens (   13.58 ms per token,    73.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.11 ms /    15 runs   (  159.87 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4739.96 ms /   187 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1683.72 ms /   119 tokens (   14.15 ms per token,    70.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2408.10 ms /    15 runs   (  160.54 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4097.53 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 59 prefix-match hit, remaining 430 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6180.01 ms /   430 tokens (   14.37 ms per token,    69.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2458.55 ms /    15 runs   (  163.90 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    8647.80 ms /   445 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2441.12 ms /   118 tokens (   20.69 ms per token,    48.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2480.30 ms /    15 runs   (  165.35 ms per token,     6.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    4927.33 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8001.05 ms /   453 tokens (   17.66 ms per token,    56.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2498.23 ms /    15 runs   (  166.55 ms per token,     6.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   10509.07 ms /   468 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1662.04 ms /   115 tokens (   14.45 ms per token,    69.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2417.73 ms /    15 runs   (  161.18 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    4085.52 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3019.32 ms /   217 tokens (   13.91 ms per token,    71.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2414.30 ms /    15 runs   (  160.95 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    5440.36 ms /   232 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1742.61 ms /   123 tokens (   14.17 ms per token,    70.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.66 ms /    15 runs   (  159.98 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4148.09 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4535.61 ms /   315 tokens (   14.40 ms per token,    69.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2426.76 ms /    15 runs   (  161.78 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    6970.37 ms /   330 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1673.88 ms /   117 tokens (   14.31 ms per token,    69.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2392.75 ms /    15 runs   (  159.52 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4072.18 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2939.53 ms /   209 tokens (   14.06 ms per token,    71.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2409.69 ms /    15 runs   (  160.65 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    5356.06 ms /   224 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1605.71 ms /   114 tokens (   14.09 ms per token,    71.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.21 ms /    15 runs   (  159.88 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4009.61 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3565.67 ms /   222 tokens (   16.06 ms per token,    62.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2411.95 ms /    15 runs   (  160.80 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    5984.52 ms /   237 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1630.80 ms /   115 tokens (   14.18 ms per token,    70.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.91 ms /    15 runs   (  159.93 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4035.42 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5816.79 ms /   390 tokens (   14.91 ms per token,    67.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2425.10 ms /    15 runs   (  161.67 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    8250.66 ms /   405 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1681.76 ms /   118 tokens (   14.25 ms per token,    70.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2392.98 ms /    15 runs   (  159.53 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4080.48 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2390.56 ms /   176 tokens (   13.58 ms per token,    73.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.21 ms /    15 runs   (  159.88 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4795.24 ms /   191 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1686.55 ms /   119 tokens (   14.17 ms per token,    70.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.08 ms /    15 runs   (  159.87 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4090.43 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3502.03 ms /   249 tokens (   14.06 ms per token,    71.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2407.33 ms /    15 runs   (  160.49 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    5916.53 ms /   264 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1678.91 ms /   124 tokens (   13.54 ms per token,    73.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2391.83 ms /    15 runs   (  159.46 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4076.53 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 64 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2613.03 ms /   187 tokens (   13.97 ms per token,    71.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.59 ms /    15 runs   (  160.24 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5023.16 ms /   202 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1758.63 ms /   125 tokens (   14.07 ms per token,    71.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.46 ms /    15 runs   (  159.56 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4158.02 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1777.66 ms /   126 tokens (   14.11 ms per token,    70.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.73 ms /    15 runs   (  160.05 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4184.20 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2295.65 ms /   165 tokens (   13.91 ms per token,    71.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.02 ms /    15 runs   (  160.27 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4705.89 ms /   180 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 105 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2696.65 ms /   190 tokens (   14.19 ms per token,    70.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2408.34 ms /    15 runs   (  160.56 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    5111.56 ms /   205 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1774.76 ms /   122 tokens (   14.55 ms per token,    68.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.40 ms /    15 runs   (  160.23 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4184.00 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2419.50 ms /   174 tokens (   13.91 ms per token,    71.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.49 ms /    15 runs   (  160.03 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4826.38 ms /   189 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1730.76 ms /   122 tokens (   14.19 ms per token,    70.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.14 ms /    15 runs   (  159.68 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4131.90 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2336.27 ms /   172 tokens (   13.58 ms per token,    73.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.13 ms /    15 runs   (  160.08 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4743.73 ms /   187 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1751.91 ms /   122 tokens (   14.36 ms per token,    69.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.58 ms /    15 runs   (  160.37 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4163.30 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2733.57 ms /   200 tokens (   13.67 ms per token,    73.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.52 ms /    15 runs   (  160.30 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5144.80 ms /   215 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1743.33 ms /   128 tokens (   13.62 ms per token,    73.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.85 ms /    15 runs   (  159.86 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4147.09 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2321.47 ms /   165 tokens (   14.07 ms per token,    71.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.28 ms /    15 runs   (  160.02 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4727.99 ms /   180 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1772.60 ms /   125 tokens (   14.18 ms per token,    70.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.49 ms /    15 runs   (  159.57 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4172.10 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 310 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4363.23 ms /   310 tokens (   14.07 ms per token,    71.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2416.99 ms /    15 runs   (  161.13 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    6788.40 ms /   325 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1863.84 ms /   127 tokens (   14.68 ms per token,    68.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.40 ms /    15 runs   (  159.69 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4265.02 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 394 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5567.52 ms /   394 tokens (   14.13 ms per token,    70.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2421.24 ms /    15 runs   (  161.42 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    7997.47 ms /   409 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1810.83 ms /   132 tokens (   13.72 ms per token,    72.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.50 ms /    15 runs   (  159.70 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4212.21 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 407 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5743.48 ms /   407 tokens (   14.11 ms per token,    70.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2435.50 ms /    15 runs   (  162.37 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    8188.06 ms /   422 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1741.17 ms /   128 tokens (   13.60 ms per token,    73.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.47 ms /    15 runs   (  159.70 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4142.48 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2733.49 ms /   197 tokens (   13.88 ms per token,    72.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.98 ms /    15 runs   (  160.40 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    5146.05 ms /   212 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1950.80 ms /   135 tokens (   14.45 ms per token,    69.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.25 ms /    15 runs   (  160.02 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4356.92 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 76 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4016.96 ms /   288 tokens (   13.95 ms per token,    71.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2409.74 ms /    15 runs   (  160.65 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6434.45 ms /   303 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1876.69 ms /   133 tokens (   14.11 ms per token,    70.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.34 ms /    15 runs   (  159.69 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4278.12 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 73 prefix-match hit, remaining 323 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4527.26 ms /   323 tokens (   14.02 ms per token,    71.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2414.84 ms /    15 runs   (  160.99 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    6950.18 ms /   338 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 52 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1693.69 ms /   124 tokens (   13.66 ms per token,    73.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2407.35 ms /    15 runs   (  160.49 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4106.79 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 272 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3796.55 ms /   272 tokens (   13.96 ms per token,    71.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2410.67 ms /    15 runs   (  160.71 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6214.83 ms /   287 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1655.72 ms /   115 tokens (   14.40 ms per token,    69.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.31 ms /    15 runs   (  159.69 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4056.80 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 53 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4263.24 ms /   305 tokens (   13.98 ms per token,    71.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2406.80 ms /    15 runs   (  160.45 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    6677.84 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 47 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1615.20 ms /   114 tokens (   14.17 ms per token,    70.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.27 ms /    15 runs   (  159.75 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4017.24 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 53 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2764.84 ms /   196 tokens (   14.11 ms per token,    70.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2439.03 ms /    15 runs   (  162.60 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    5210.51 ms /   211 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1700.66 ms /   119 tokens (   14.29 ms per token,    69.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.70 ms /    15 runs   (  159.65 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4101.13 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 318 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4456.26 ms /   318 tokens (   14.01 ms per token,    71.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2416.63 ms /    15 runs   (  161.11 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    6880.91 ms /   333 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1689.12 ms /   119 tokens (   14.19 ms per token,    70.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.51 ms /    15 runs   (  159.77 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4091.23 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 56 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5479.10 ms /   390 tokens (   14.05 ms per token,    71.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2421.17 ms /    15 runs   (  161.41 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    7908.98 ms /   405 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1753.23 ms /   124 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.54 ms /    15 runs   (  159.77 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4155.46 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 61 prefix-match hit, remaining 180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2487.85 ms /   180 tokens (   13.82 ms per token,    72.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.54 ms /    15 runs   (  160.10 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4895.82 ms /   195 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1696.50 ms /   117 tokens (   14.50 ms per token,    68.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.89 ms /    15 runs   (  159.93 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4101.14 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 54 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2018.96 ms /   141 tokens (   14.32 ms per token,    69.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.28 ms /    15 runs   (  160.35 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4430.20 ms /   156 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1688.62 ms /   118 tokens (   14.31 ms per token,    69.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.23 ms /    15 runs   (  160.02 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4094.59 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 687 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =   10363.34 ms /   687 tokens (   15.08 ms per token,    66.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2443.40 ms /    15 runs   (  162.89 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   12819.03 ms /   702 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1871.03 ms /   130 tokens (   14.39 ms per token,    69.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.31 ms /    15 runs   (  159.82 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4274.25 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2089.11 ms /   147 tokens (   14.21 ms per token,    70.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.74 ms /    15 runs   (  160.32 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4499.96 ms /   162 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1739.19 ms /   121 tokens (   14.37 ms per token,    69.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.42 ms /    15 runs   (  159.96 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4144.41 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2399.49 ms /   170 tokens (   14.11 ms per token,    70.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.68 ms /    15 runs   (  160.38 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4811.43 ms /   185 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1658.09 ms /   120 tokens (   13.82 ms per token,    72.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.86 ms /    15 runs   (  159.86 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4061.72 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2254.06 ms /   160 tokens (   14.09 ms per token,    70.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2408.08 ms /    15 runs   (  160.54 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4668.29 ms /   175 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1802.09 ms /   125 tokens (   14.42 ms per token,    69.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.26 ms /    15 runs   (  160.08 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4209.13 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 171 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2389.42 ms /   171 tokens (   13.97 ms per token,    71.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.30 ms /    15 runs   (  160.29 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4800.33 ms /   186 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 48 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1855.95 ms /   131 tokens (   14.17 ms per token,    70.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.71 ms /    15 runs   (  159.98 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4261.53 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2456.24 ms /   175 tokens (   14.04 ms per token,    71.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2450.49 ms /    15 runs   (  163.37 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    4913.34 ms /   190 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1768.68 ms /   121 tokens (   14.62 ms per token,    68.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.76 ms /    15 runs   (  160.05 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4175.18 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 317 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4538.50 ms /   317 tokens (   14.32 ms per token,    69.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2415.85 ms /    15 runs   (  161.06 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    6962.38 ms /   332 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1684.95 ms /   117 tokens (   14.40 ms per token,    69.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.04 ms /    15 runs   (  159.87 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4088.86 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 54 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2955.97 ms /   210 tokens (   14.08 ms per token,    71.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2411.57 ms /    15 runs   (  160.77 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    5374.15 ms /   225 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1773.84 ms /   123 tokens (   14.42 ms per token,    69.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.80 ms /    15 runs   (  159.92 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4178.48 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3095.74 ms /   217 tokens (   14.27 ms per token,    70.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2410.86 ms /    15 runs   (  160.72 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    5513.36 ms /   232 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1752.08 ms /   119 tokens (   14.72 ms per token,    67.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.79 ms /    15 runs   (  160.05 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4158.59 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 526 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7611.65 ms /   526 tokens (   14.47 ms per token,    69.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2430.82 ms /    15 runs   (  162.05 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   10052.39 ms /   541 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2290.51 ms /   118 tokens (   19.41 ms per token,    51.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.31 ms /    15 runs   (  160.02 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4696.61 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3227.24 ms /   226 tokens (   14.28 ms per token,    70.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2424.54 ms /    15 runs   (  161.64 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    5658.78 ms /   241 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1661.97 ms /   120 tokens (   13.85 ms per token,    72.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.08 ms /    15 runs   (  159.94 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4066.91 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 566 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8146.04 ms /   566 tokens (   14.39 ms per token,    69.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2430.82 ms /    15 runs   (  162.05 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   10587.49 ms /   581 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1771.96 ms /   121 tokens (   14.64 ms per token,    68.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.63 ms /    15 runs   (  159.98 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4177.37 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2959.25 ms /   209 tokens (   14.16 ms per token,    70.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2412.57 ms /    15 runs   (  160.84 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    5378.42 ms /   224 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1826.78 ms /   126 tokens (   14.50 ms per token,    68.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2409.61 ms /    15 runs   (  160.64 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4242.29 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3407.81 ms /   244 tokens (   13.97 ms per token,    71.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2413.47 ms /    15 runs   (  160.90 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    5828.10 ms /   259 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 47 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1762.18 ms /   121 tokens (   14.56 ms per token,    68.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.55 ms /    15 runs   (  160.04 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4168.35 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 401 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5716.59 ms /   401 tokens (   14.26 ms per token,    70.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2432.26 ms /    15 runs   (  162.15 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    8157.43 ms /   416 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1789.16 ms /   128 tokens (   13.98 ms per token,    71.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.79 ms /    15 runs   (  159.99 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4194.69 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 402 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5704.46 ms /   402 tokens (   14.19 ms per token,    70.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2426.95 ms /    15 runs   (  161.80 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    8140.18 ms /   417 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1656.04 ms /   120 tokens (   13.80 ms per token,    72.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.62 ms /    15 runs   (  160.11 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4063.55 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2365.83 ms /   167 tokens (   14.17 ms per token,    70.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.86 ms /    15 runs   (  160.39 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4778.26 ms /   182 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1617.83 ms /   116 tokens (   13.95 ms per token,    71.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.46 ms /    15 runs   (  159.96 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4023.02 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 53 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2474.99 ms /   175 tokens (   14.14 ms per token,    70.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2407.40 ms /    15 runs   (  160.49 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4888.66 ms /   190 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1764.42 ms /   122 tokens (   14.46 ms per token,    69.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.86 ms /    15 runs   (  160.12 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4172.26 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 59 prefix-match hit, remaining 433 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6209.79 ms /   433 tokens (   14.34 ms per token,    69.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2427.52 ms /    15 runs   (  161.83 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    8646.31 ms /   448 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1763.94 ms /   121 tokens (   14.58 ms per token,    68.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.15 ms /    15 runs   (  160.08 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4170.89 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6486.74 ms /   456 tokens (   14.23 ms per token,    70.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2432.75 ms /    15 runs   (  162.18 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    8929.01 ms /   471 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1702.11 ms /   118 tokens (   14.42 ms per token,    69.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.26 ms /    15 runs   (  160.15 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4110.03 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3054.78 ms /   220 tokens (   13.89 ms per token,    72.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2417.10 ms /    15 runs   (  161.14 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    5478.72 ms /   235 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1857.42 ms /   126 tokens (   14.74 ms per token,    67.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.93 ms /    15 runs   (  160.20 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4266.08 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 318 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4496.13 ms /   318 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2422.42 ms /    15 runs   (  161.49 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    6926.55 ms /   333 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1674.22 ms /   120 tokens (   13.95 ms per token,    71.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2408.12 ms /    15 runs   (  160.54 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4088.18 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3450.42 ms /   212 tokens (   16.28 ms per token,    61.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2422.00 ms /    15 runs   (  161.47 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    5879.12 ms /   227 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1718.07 ms /   117 tokens (   14.68 ms per token,    68.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.68 ms /    15 runs   (  160.11 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4125.63 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 54 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3198.16 ms /   225 tokens (   14.21 ms per token,    70.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2413.92 ms /    15 runs   (  160.93 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    5618.97 ms /   240 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1728.22 ms /   118 tokens (   14.65 ms per token,    68.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.69 ms /    15 runs   (  160.31 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4138.63 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 393 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5628.17 ms /   393 tokens (   14.32 ms per token,    69.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2427.27 ms /    15 runs   (  161.82 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    8064.21 ms /   408 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1783.56 ms /   121 tokens (   14.74 ms per token,    67.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2407.98 ms /    15 runs   (  160.53 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4197.49 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 179 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2566.65 ms /   179 tokens (   14.34 ms per token,    69.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2411.37 ms /    15 runs   (  160.76 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    4984.58 ms /   194 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1784.41 ms /   122 tokens (   14.63 ms per token,    68.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.56 ms /    15 runs   (  160.30 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4194.87 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 252 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3554.48 ms /   252 tokens (   14.11 ms per token,    70.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2422.25 ms /    15 runs   (  161.48 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    5983.89 ms /   267 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1863.88 ms /   127 tokens (   14.68 ms per token,    68.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2407.53 ms /    15 runs   (  160.50 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4277.32 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 64 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2736.28 ms /   190 tokens (   14.40 ms per token,    69.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2458.25 ms /    15 runs   (  163.88 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    5201.25 ms /   205 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1782.40 ms /   128 tokens (   13.92 ms per token,    71.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2406.54 ms /    15 runs   (  160.44 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4194.92 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1916.37 ms /   129 tokens (   14.86 ms per token,    67.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2413.54 ms /    15 runs   (  160.90 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    4335.81 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 168 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2315.64 ms /   168 tokens (   13.78 ms per token,    72.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2409.46 ms /    15 runs   (  160.63 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4731.29 ms /   183 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 105 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2775.48 ms /   193 tokens (   14.38 ms per token,    69.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2413.59 ms /    15 runs   (  160.91 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    5195.56 ms /   208 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1846.82 ms /   125 tokens (   14.77 ms per token,    67.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.74 ms /    15 runs   (  160.12 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4254.30 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2521.93 ms /   177 tokens (   14.25 ms per token,    70.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2408.60 ms /    15 runs   (  160.57 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4936.96 ms /   192 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1812.32 ms /   125 tokens (   14.50 ms per token,    68.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2400.69 ms /    15 runs   (  160.05 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4218.90 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2535.38 ms /   175 tokens (   14.49 ms per token,    69.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2408.80 ms /    15 runs   (  160.59 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4950.47 ms /   190 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1825.78 ms /   125 tokens (   14.61 ms per token,    68.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.17 ms /    15 runs   (  160.21 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4234.87 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2848.24 ms /   203 tokens (   14.03 ms per token,    71.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2412.69 ms /    15 runs   (  160.85 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    5267.42 ms /   218 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1931.56 ms /   131 tokens (   14.74 ms per token,    67.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2392.23 ms /    15 runs   (  159.48 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    4329.70 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 168 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2274.50 ms /   168 tokens (   13.54 ms per token,    73.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.85 ms /    15 runs   (  159.92 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4679.62 ms /   183 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1968.40 ms /   128 tokens (   15.38 ms per token,    65.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.69 ms /    15 runs   (  158.58 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4352.86 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 313 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4274.04 ms /   313 tokens (   13.66 ms per token,    73.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.68 ms /    15 runs   (  158.98 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    6666.65 ms /   328 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1776.07 ms /   130 tokens (   13.66 ms per token,    73.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.99 ms /    15 runs   (  158.27 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4155.88 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 397 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5387.66 ms /   397 tokens (   13.57 ms per token,    73.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.81 ms /    15 runs   (  159.19 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7783.98 ms /   412 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1827.45 ms /   135 tokens (   13.54 ms per token,    73.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.52 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4199.90 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 72 prefix-match hit, remaining 410 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5670.83 ms /   410 tokens (   13.83 ms per token,    72.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2386.93 ms /    15 runs   (  159.13 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8067.11 ms /   425 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1784.23 ms /   131 tokens (   13.62 ms per token,    73.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.55 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4157.62 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2618.59 ms /   200 tokens (   13.09 ms per token,    76.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.56 ms /    15 runs   (  158.70 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    5005.75 ms /   215 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1887.28 ms /   138 tokens (   13.68 ms per token,    73.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.80 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4260.84 ms /   153 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 76 prefix-match hit, remaining 291 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3965.10 ms /   291 tokens (   13.63 ms per token,    73.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.30 ms /    15 runs   (  158.62 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6351.89 ms /   306 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1775.61 ms /   136 tokens (   13.06 ms per token,    76.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.93 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4148.33 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 73 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4406.01 ms /   326 tokens (   13.52 ms per token,    73.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.45 ms /    15 runs   (  158.96 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    6798.67 ms /   341 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 52 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1730.97 ms /   127 tokens (   13.63 ms per token,    73.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.18 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4103.00 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 275 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3714.09 ms /   275 tokens (   13.51 ms per token,    74.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2381.08 ms /    15 runs   (  158.74 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6102.47 ms /   290 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1705.35 ms /   121 tokens (   14.09 ms per token,    70.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.34 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4077.47 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 53 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4199.40 ms /   311 tokens (   13.50 ms per token,    74.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.41 ms /    15 runs   (  158.56 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    6585.19 ms /   326 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 47 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1565.34 ms /   120 tokens (   13.04 ms per token,    76.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.54 ms /    15 runs   (  157.64 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3935.63 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 53 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2747.78 ms /   202 tokens (   13.60 ms per token,    73.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.23 ms /    15 runs   (  160.35 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5159.61 ms /   217 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1726.27 ms /   125 tokens (   13.81 ms per token,    72.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.21 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4099.47 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 324 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4317.95 ms /   324 tokens (   13.33 ms per token,    75.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.78 ms /    15 runs   (  158.99 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    6710.59 ms /   339 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1723.47 ms /   125 tokens (   13.79 ms per token,    72.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.53 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4094.85 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 56 prefix-match hit, remaining 396 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5330.12 ms /   396 tokens (   13.46 ms per token,    74.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.89 ms /    15 runs   (  159.19 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7726.94 ms /   411 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1775.96 ms /   130 tokens (   13.66 ms per token,    73.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.22 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4147.96 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 61 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2516.10 ms /   186 tokens (   13.53 ms per token,    73.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.50 ms /    15 runs   (  158.23 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4896.03 ms /   201 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1689.88 ms /   123 tokens (   13.74 ms per token,    72.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.53 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4060.91 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 54 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1986.31 ms /   147 tokens (   13.51 ms per token,    74.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.78 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4363.03 ms /   162 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1643.96 ms /   124 tokens (   13.26 ms per token,    75.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.81 ms /    15 runs   (  157.65 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4014.65 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 693 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    9469.91 ms /   693 tokens (   13.67 ms per token,    73.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2402.87 ms /    15 runs   (  160.19 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   11884.97 ms /   708 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1779.58 ms /   136 tokens (   13.09 ms per token,    76.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.52 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4152.93 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 67 prefix-match hit, remaining 153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2087.98 ms /   153 tokens (   13.65 ms per token,    73.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.79 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4464.82 ms /   168 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1728.57 ms /   127 tokens (   13.61 ms per token,    73.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.71 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4101.97 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 68 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2296.32 ms /   176 tokens (   13.05 ms per token,    76.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.60 ms /    15 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4674.26 ms /   191 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1716.07 ms /   126 tokens (   13.62 ms per token,    73.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.70 ms /    15 runs   (  158.25 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4095.51 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 67 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2282.95 ms /   166 tokens (   13.75 ms per token,    72.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2421.17 ms /    15 runs   (  161.41 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    4710.70 ms /   181 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1810.30 ms /   131 tokens (   13.82 ms per token,    72.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.03 ms /    15 runs   (  157.67 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4181.06 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2397.45 ms /   177 tokens (   13.54 ms per token,    73.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.91 ms /    15 runs   (  158.19 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4776.70 ms /   192 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 48 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1860.25 ms /   137 tokens (   13.58 ms per token,    73.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.66 ms /    15 runs   (  157.91 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4234.78 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2445.07 ms /   181 tokens (   13.51 ms per token,    74.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.49 ms /    15 runs   (  158.43 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4827.92 ms /   196 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1759.31 ms /   127 tokens (   13.85 ms per token,    72.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.99 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4131.02 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 323 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4354.11 ms /   323 tokens (   13.48 ms per token,    74.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2389.73 ms /    15 runs   (  159.32 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    6751.72 ms /   338 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1674.68 ms /   123 tokens (   13.62 ms per token,    73.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.42 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4047.84 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 54 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2869.99 ms /   216 tokens (   13.29 ms per token,    75.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.39 ms /    15 runs   (  158.56 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5255.08 ms /   231 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1767.77 ms /   129 tokens (   13.70 ms per token,    72.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.76 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4139.11 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3004.43 ms /   223 tokens (   13.47 ms per token,    74.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.14 ms /    15 runs   (  158.48 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5388.27 ms /   238 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1747.79 ms /   125 tokens (   13.98 ms per token,    71.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.95 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4120.61 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 532 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7206.33 ms /   532 tokens (   13.55 ms per token,    73.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.90 ms /    15 runs   (  159.59 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    9610.06 ms /   547 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1619.18 ms /   124 tokens (   13.06 ms per token,    76.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.07 ms /    15 runs   (  157.67 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3990.06 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3065.98 ms /   232 tokens (   13.22 ms per token,    75.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.07 ms /    15 runs   (  158.34 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    5447.95 ms /   247 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1725.16 ms /   126 tokens (   13.69 ms per token,    73.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.26 ms /    15 runs   (  157.68 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4096.04 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 572 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7908.28 ms /   572 tokens (   13.83 ms per token,    72.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.80 ms /    15 runs   (  159.92 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   10317.79 ms /   587 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1721.47 ms /   127 tokens (   13.55 ms per token,    73.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.95 ms /    15 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4095.17 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2933.71 ms /   215 tokens (   13.65 ms per token,    73.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.45 ms /    15 runs   (  158.56 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5318.94 ms /   230 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1715.71 ms /   132 tokens (   13.00 ms per token,    76.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.81 ms /    15 runs   (  157.65 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4086.29 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 250 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3373.22 ms /   250 tokens (   13.49 ms per token,    74.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.36 ms /    15 runs   (  158.56 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5758.61 ms /   265 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 47 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1728.51 ms /   127 tokens (   13.61 ms per token,    73.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.62 ms /    15 runs   (  157.64 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4098.53 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 407 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5553.69 ms /   407 tokens (   13.65 ms per token,    73.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2388.09 ms /    15 runs   (  159.21 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7950.74 ms /   422 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1821.37 ms /   134 tokens (   13.59 ms per token,    73.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.39 ms /    15 runs   (  158.69 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    4207.60 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 65 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5487.03 ms /   408 tokens (   13.45 ms per token,    74.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2388.25 ms /    15 runs   (  159.22 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7884.24 ms /   423 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1717.61 ms /   126 tokens (   13.63 ms per token,    73.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.84 ms /    15 runs   (  157.66 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4088.17 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 173 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2342.73 ms /   173 tokens (   13.54 ms per token,    73.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.54 ms /    15 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4719.62 ms /   188 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1723.79 ms /   122 tokens (   14.13 ms per token,    70.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.83 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4095.43 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 53 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2482.94 ms /   181 tokens (   13.72 ms per token,    72.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.84 ms /    15 runs   (  158.19 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4862.24 ms /   196 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1671.39 ms /   128 tokens (   13.06 ms per token,    76.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.27 ms /    15 runs   (  157.68 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4042.33 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 59 prefix-match hit, remaining 439 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6056.97 ms /   439 tokens (   13.80 ms per token,    72.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2388.28 ms /    15 runs   (  159.22 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8454.66 ms /   454 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1732.37 ms /   127 tokens (   13.64 ms per token,    73.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.18 ms /    15 runs   (  157.68 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4103.14 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6317.00 ms /   462 tokens (   13.67 ms per token,    73.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2391.67 ms /    15 runs   (  159.44 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    8718.01 ms /   477 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1640.11 ms /   124 tokens (   13.23 ms per token,    75.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.92 ms /    15 runs   (  157.66 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4010.63 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3052.20 ms /   226 tokens (   13.51 ms per token,    74.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.96 ms /    15 runs   (  158.53 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5436.97 ms /   241 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1725.44 ms /   132 tokens (   13.07 ms per token,    76.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.40 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4098.68 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 324 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4312.72 ms /   324 tokens (   13.31 ms per token,    75.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2385.35 ms /    15 runs   (  159.02 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    6706.03 ms /   339 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1753.77 ms /   126 tokens (   13.92 ms per token,    71.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.72 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4125.33 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2981.74 ms /   218 tokens (   13.68 ms per token,    73.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.41 ms /    15 runs   (  158.49 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5365.87 ms /   233 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1675.22 ms /   123 tokens (   13.62 ms per token,    73.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.04 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4049.00 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 54 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3144.25 ms /   231 tokens (   13.61 ms per token,    73.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.74 ms /    15 runs   (  158.52 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5528.85 ms /   246 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1627.99 ms /   124 tokens (   13.13 ms per token,    76.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.25 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4000.02 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 399 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5406.92 ms /   399 tokens (   13.55 ms per token,    73.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.03 ms /    15 runs   (  159.14 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7802.44 ms /   414 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1724.79 ms /   127 tokens (   13.58 ms per token,    73.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.40 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4096.78 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 58 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2494.18 ms /   185 tokens (   13.48 ms per token,    74.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.68 ms /    15 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4872.30 ms /   200 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1730.59 ms /   128 tokens (   13.52 ms per token,    73.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.34 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4101.70 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 60 prefix-match hit, remaining 258 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3477.68 ms /   258 tokens (   13.48 ms per token,    74.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.39 ms /    15 runs   (  158.63 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    5864.12 ms /   273 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1850.23 ms /   133 tokens (   13.91 ms per token,    71.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.32 ms /    15 runs   (  157.82 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4223.40 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 64 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2564.64 ms /   196 tokens (   13.08 ms per token,    76.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.48 ms /    15 runs   (  158.37 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4946.58 ms /   211 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1815.78 ms /   134 tokens (   13.55 ms per token,    73.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.46 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4188.04 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 65 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1859.49 ms /   135 tokens (   13.77 ms per token,    72.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.37 ms /    15 runs   (  158.09 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4236.63 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2872.70 ms /   174 tokens (   16.51 ms per token,    60.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.65 ms /    15 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    5250.69 ms /   189 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 105 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2686.46 ms /   199 tokens (   13.50 ms per token,    74.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.84 ms /    15 runs   (  158.39 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5068.79 ms /   214 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1807.10 ms /   131 tokens (   13.79 ms per token,    72.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.77 ms /    15 runs   (  157.65 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4177.62 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2497.81 ms /   183 tokens (   13.65 ms per token,    73.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.26 ms /    15 runs   (  158.15 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4876.35 ms /   198 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1785.98 ms /   131 tokens (   13.63 ms per token,    73.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.28 ms /    15 runs   (  157.95 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4161.02 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2529.70 ms /   181 tokens (   13.98 ms per token,    71.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.20 ms /    15 runs   (  158.35 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4911.31 ms /   196 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1774.12 ms /   131 tokens (   13.54 ms per token,    73.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.65 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4147.54 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2818.57 ms /   209 tokens (   13.49 ms per token,    74.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.59 ms /    15 runs   (  158.57 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5203.85 ms /   224 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1859.79 ms /   137 tokens (   13.58 ms per token,    73.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.85 ms /    15 runs   (  158.26 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4239.24 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 68 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2379.23 ms /   174 tokens (   13.67 ms per token,    73.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.33 ms /    15 runs   (  158.16 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4757.83 ms /   189 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1819.36 ms /   134 tokens (   13.58 ms per token,    73.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.93 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4192.09 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 65 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4312.25 ms /   319 tokens (   13.52 ms per token,    73.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.18 ms /    15 runs   (  158.95 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    6704.10 ms /   334 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1794.16 ms /   136 tokens (   13.19 ms per token,    75.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.19 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4166.12 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 67 prefix-match hit, remaining 403 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5497.63 ms /   403 tokens (   13.64 ms per token,    73.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2388.01 ms /    15 runs   (  159.20 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7894.35 ms /   418 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1914.28 ms /   141 tokens (   13.58 ms per token,    73.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.52 ms /    15 runs   (  157.97 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4289.57 ms /   156 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 72 prefix-match hit, remaining 416 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5954.90 ms /   416 tokens (   14.31 ms per token,    69.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.65 ms /    15 runs   (  159.18 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8351.59 ms /   431 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1869.31 ms /   137 tokens (   13.64 ms per token,    73.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.84 ms /    15 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4243.00 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 68 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2779.92 ms /   206 tokens (   13.49 ms per token,    74.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.13 ms /    15 runs   (  158.41 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5162.66 ms /   221 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1885.16 ms /   144 tokens (   13.09 ms per token,    76.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.77 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4261.91 ms /   159 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 76 prefix-match hit, remaining 297 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4036.99 ms /   297 tokens (   13.59 ms per token,    73.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.83 ms /    15 runs   (  158.72 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6425.60 ms /   312 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1931.08 ms /   142 tokens (   13.60 ms per token,    73.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.90 ms /    15 runs   (  157.93 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4305.90 ms /   157 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 73 prefix-match hit, remaining 332 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4460.50 ms /   332 tokens (   13.44 ms per token,    74.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.47 ms /    15 runs   (  158.96 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    6853.13 ms /   347 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 52 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1823.82 ms /   133 tokens (   13.71 ms per token,    72.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.56 ms /    15 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4200.19 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3829.61 ms /   281 tokens (   13.63 ms per token,    73.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2382.01 ms /    15 runs   (  158.80 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6219.17 ms /   296 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1450.27 ms /   105 tokens (   13.81 ms per token,    72.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.91 ms /    15 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3823.84 ms /   120 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 295 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3972.31 ms /   295 tokens (   13.47 ms per token,    74.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.11 ms /    15 runs   (  158.67 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6359.98 ms /   310 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 47 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1364.29 ms /   104 tokens (   13.12 ms per token,    76.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.83 ms /    15 runs   (  157.79 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3736.70 ms /   119 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2552.19 ms /   186 tokens (   13.72 ms per token,    72.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.62 ms /    15 runs   (  158.17 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4931.24 ms /   201 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1496.55 ms /   109 tokens (   13.73 ms per token,    72.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.03 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3869.26 ms /   124 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 308 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4115.02 ms /   308 tokens (   13.36 ms per token,    74.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.05 ms /    15 runs   (  158.67 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6503.02 ms /   323 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1494.64 ms /   109 tokens (   13.71 ms per token,    72.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.37 ms /    15 runs   (  157.82 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3867.63 ms /   124 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 56 prefix-match hit, remaining 380 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5127.64 ms /   380 tokens (   13.49 ms per token,    74.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.82 ms /    15 runs   (  158.99 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    7520.66 ms /   395 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1552.35 ms /   114 tokens (   13.62 ms per token,    73.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.86 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3923.74 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 61 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2293.56 ms /   170 tokens (   13.49 ms per token,    74.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.93 ms /    15 runs   (  158.00 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4669.70 ms /   185 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1515.47 ms /   107 tokens (   14.16 ms per token,    70.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.31 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3887.29 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1824.69 ms /   131 tokens (   13.93 ms per token,    71.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.95 ms /    15 runs   (  158.20 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4203.35 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1448.64 ms /   108 tokens (   13.41 ms per token,    74.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.73 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3821.94 ms /   123 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 677 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    9213.51 ms /   677 tokens (   13.61 ms per token,    73.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.86 ms /    15 runs   (  160.26 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   11629.07 ms /   692 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1573.89 ms /   120 tokens (   13.12 ms per token,    76.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.59 ms /    15 runs   (  157.64 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3944.26 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1899.90 ms /   137 tokens (   13.87 ms per token,    72.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.12 ms /    15 runs   (  158.07 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4276.97 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1597.34 ms /   111 tokens (   14.39 ms per token,    69.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.60 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3970.55 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2112.27 ms /   160 tokens (   13.20 ms per token,    75.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.33 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4488.75 ms /   175 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 55 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1520.20 ms /   110 tokens (   13.82 ms per token,    72.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.33 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3892.17 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2063.04 ms /   150 tokens (   13.75 ms per token,    72.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.28 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4439.28 ms /   165 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1568.08 ms /   115 tokens (   13.64 ms per token,    73.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.32 ms /    15 runs   (  158.09 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3945.11 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2181.99 ms /   161 tokens (   13.55 ms per token,    73.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.82 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4558.96 ms /   176 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 48 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1665.31 ms /   121 tokens (   13.76 ms per token,    72.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.52 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4037.58 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2262.93 ms /   165 tokens (   13.71 ms per token,    72.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.14 ms /    15 runs   (  158.14 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4641.29 ms /   180 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1524.05 ms /   111 tokens (   13.73 ms per token,    72.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.87 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3895.49 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4159.69 ms /   307 tokens (   13.55 ms per token,    73.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.25 ms /    15 runs   (  158.62 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6546.70 ms /   322 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1468.59 ms /   107 tokens (   13.73 ms per token,    72.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.75 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3840.93 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2644.74 ms /   200 tokens (   13.22 ms per token,    75.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.81 ms /    15 runs   (  158.39 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5027.10 ms /   215 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1551.10 ms /   113 tokens (   13.73 ms per token,    72.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.54 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3922.37 ms /   128 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 60 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3004.70 ms /   207 tokens (   14.52 ms per token,    68.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.91 ms /    15 runs   (  158.53 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5389.13 ms /   222 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1496.45 ms /   109 tokens (   13.73 ms per token,    72.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.60 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3868.70 ms /   124 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 516 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7134.29 ms /   516 tokens (   13.83 ms per token,    72.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.85 ms /    15 runs   (  159.66 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    9539.20 ms /   531 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1411.98 ms /   108 tokens (   13.07 ms per token,    76.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.57 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3784.11 ms /   123 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2880.15 ms /   216 tokens (   13.33 ms per token,    75.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.41 ms /    15 runs   (  158.56 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5265.36 ms /   231 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1508.73 ms /   110 tokens (   13.72 ms per token,    72.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.15 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3881.44 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 556 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7525.78 ms /   556 tokens (   13.54 ms per token,    73.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2398.03 ms /    15 runs   (  159.87 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    9933.78 ms /   571 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1514.35 ms /   111 tokens (   13.64 ms per token,    73.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.93 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3885.81 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2725.56 ms /   199 tokens (   13.70 ms per token,    73.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2428.09 ms /    15 runs   (  161.87 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    5160.36 ms /   214 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1518.82 ms /   116 tokens (   13.09 ms per token,    76.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.36 ms /    15 runs   (  158.16 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3896.82 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3153.34 ms /   234 tokens (   13.48 ms per token,    74.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.50 ms /    15 runs   (  158.37 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5535.77 ms /   249 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 47 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1537.14 ms /   111 tokens (   13.85 ms per token,    72.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.88 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3908.40 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 60 prefix-match hit, remaining 391 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5304.00 ms /   391 tokens (   13.57 ms per token,    73.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.49 ms /    15 runs   (  159.17 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7700.19 ms /   406 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1656.34 ms /   118 tokens (   14.04 ms per token,    71.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.93 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4028.06 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 392 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5273.77 ms /   392 tokens (   13.45 ms per token,    74.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2391.47 ms /    15 runs   (  159.43 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    7674.08 ms /   407 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1503.37 ms /   110 tokens (   13.67 ms per token,    73.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.68 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3875.78 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 157 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2171.39 ms /   157 tokens (   13.83 ms per token,    72.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.47 ms /    15 runs   (  158.10 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4548.97 ms /   172 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1468.31 ms /   106 tokens (   13.85 ms per token,    72.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.62 ms /    15 runs   (  158.17 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3846.65 ms /   121 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 53 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2240.00 ms /   165 tokens (   13.58 ms per token,    73.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.31 ms /    15 runs   (  158.22 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4619.42 ms /   180 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1460.38 ms /   112 tokens (   13.04 ms per token,    76.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.66 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3833.79 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 59 prefix-match hit, remaining 423 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5765.74 ms /   423 tokens (   13.63 ms per token,    73.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.77 ms /    15 runs   (  159.18 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8162.60 ms /   438 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1516.17 ms /   111 tokens (   13.66 ms per token,    73.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.43 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3887.19 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 446 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6071.91 ms /   446 tokens (   13.61 ms per token,    73.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2389.73 ms /    15 runs   (  159.32 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8470.99 ms /   461 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1405.72 ms /   108 tokens (   13.02 ms per token,    76.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.80 ms /    15 runs   (  158.12 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3783.10 ms /   123 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2839.96 ms /   210 tokens (   13.52 ms per token,    73.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.74 ms /    15 runs   (  158.38 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5222.27 ms /   225 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1518.57 ms /   116 tokens (   13.09 ms per token,    76.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.49 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3889.73 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 308 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4187.99 ms /   308 tokens (   13.60 ms per token,    73.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2381.29 ms /    15 runs   (  158.75 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6577.16 ms /   323 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1506.71 ms /   110 tokens (   13.70 ms per token,    73.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.11 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3880.64 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2719.85 ms /   202 tokens (   13.46 ms per token,    74.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.82 ms /    15 runs   (  158.45 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5103.15 ms /   217 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1483.49 ms /   107 tokens (   13.86 ms per token,    72.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.10 ms /    15 runs   (  160.27 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3893.52 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2919.47 ms /   215 tokens (   13.58 ms per token,    73.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.10 ms /    15 runs   (  158.54 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5304.32 ms /   230 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1414.25 ms /   108 tokens (   13.09 ms per token,    76.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.30 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3784.93 ms /   123 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 55 prefix-match hit, remaining 383 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5179.25 ms /   383 tokens (   13.52 ms per token,    73.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2385.32 ms /    15 runs   (  159.02 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    7573.60 ms /   398 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1515.97 ms /   111 tokens (   13.66 ms per token,    73.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.08 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3889.30 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 58 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2286.60 ms /   169 tokens (   13.53 ms per token,    73.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.95 ms /    15 runs   (  158.13 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4664.75 ms /   184 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1491.88 ms /   112 tokens (   13.32 ms per token,    75.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.44 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3863.93 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 60 prefix-match hit, remaining 242 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3267.15 ms /   242 tokens (   13.50 ms per token,    74.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2374.99 ms /    15 runs   (  158.33 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    5649.16 ms /   257 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1605.46 ms /   117 tokens (   13.72 ms per token,    72.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.72 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3976.92 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 64 prefix-match hit, remaining 180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2949.43 ms /   180 tokens (   16.39 ms per token,    61.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.64 ms /    15 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    5327.40 ms /   195 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1619.38 ms /   118 tokens (   13.72 ms per token,    72.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.53 ms /    15 runs   (  158.10 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3996.64 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1702.52 ms /   119 tokens (   14.31 ms per token,    69.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.01 ms /    15 runs   (  157.93 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4077.29 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 158 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2190.73 ms /   158 tokens (   13.87 ms per token,    72.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.15 ms /    15 runs   (  158.01 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4567.05 ms /   173 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 105 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2471.73 ms /   183 tokens (   13.51 ms per token,    74.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.31 ms /    15 runs   (  158.35 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4853.27 ms /   198 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1569.75 ms /   115 tokens (   13.65 ms per token,    73.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.74 ms /    15 runs   (  157.65 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3939.80 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2484.61 ms /   167 tokens (   14.88 ms per token,    67.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2374.64 ms /    15 runs   (  158.31 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4865.53 ms /   182 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1574.61 ms /   115 tokens (   13.69 ms per token,    73.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.97 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3947.19 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 62 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2263.50 ms /   165 tokens (   13.72 ms per token,    72.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.74 ms /    15 runs   (  158.18 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4642.59 ms /   180 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1573.51 ms /   115 tokens (   13.68 ms per token,    73.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.42 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3944.58 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2636.49 ms /   193 tokens (   13.66 ms per token,    73.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.35 ms /    15 runs   (  158.56 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5021.32 ms /   208 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1692.60 ms /   121 tokens (   13.99 ms per token,    71.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.01 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4064.44 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 158 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2143.55 ms /   158 tokens (   13.57 ms per token,    73.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.48 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4519.80 ms /   173 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1624.02 ms /   118 tokens (   13.76 ms per token,    72.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.65 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3996.45 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 65 prefix-match hit, remaining 303 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4164.25 ms /   303 tokens (   13.74 ms per token,    72.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.20 ms /    15 runs   (  158.68 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6552.27 ms /   318 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1591.20 ms /   120 tokens (   13.26 ms per token,    75.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.86 ms /    15 runs   (  157.79 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3963.75 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5242.00 ms /   387 tokens (   13.55 ms per token,    73.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2389.02 ms /    15 runs   (  159.27 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7639.77 ms /   402 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1710.54 ms /   125 tokens (   13.68 ms per token,    73.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.95 ms /    15 runs   (  157.66 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4081.34 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 400 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5354.54 ms /   400 tokens (   13.39 ms per token,    74.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.99 ms /    15 runs   (  159.20 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    7751.52 ms /   415 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1681.58 ms /   121 tokens (   13.90 ms per token,    71.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.05 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4055.57 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2583.24 ms /   190 tokens (   13.60 ms per token,    73.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.28 ms /    15 runs   (  158.49 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4966.95 ms /   205 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1706.46 ms /   128 tokens (   13.33 ms per token,    75.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.92 ms /    15 runs   (  157.66 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4077.14 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 76 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3807.38 ms /   281 tokens (   13.55 ms per token,    73.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.77 ms /    15 runs   (  158.58 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    6193.79 ms /   296 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1725.84 ms /   126 tokens (   13.70 ms per token,    73.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.81 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4097.45 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 73 prefix-match hit, remaining 316 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    4752.72 ms /   316 tokens (   15.04 ms per token,    66.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.72 ms /    15 runs   (  158.98 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    7145.46 ms /   331 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 52 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1654.90 ms /   117 tokens (   14.14 ms per token,    70.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.43 ms /    15 runs   (  158.16 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4033.15 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    3586.31 ms /   265 tokens (   13.53 ms per token,    73.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.04 ms /    15 runs   (  158.67 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    5973.56 ms /   280 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1314.13 ms /    93 tokens (   14.13 ms per token,    70.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.11 ms /    15 runs   (  158.01 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3689.76 ms /   108 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1158.97 ms /    88 tokens (   13.17 ms per token,    75.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.76 ms /    15 runs   (  158.45 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3541.07 ms /   103 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1317.00 ms /    95 tokens (   13.86 ms per token,    72.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.72 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3690.09 ms /   110 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1207.19 ms /    92 tokens (   13.12 ms per token,    76.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.25 ms /    15 runs   (  158.42 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3588.90 ms /   107 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1203.61 ms /    92 tokens (   13.08 ms per token,    76.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.64 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3576.64 ms /   107 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 74 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1414.56 ms /    99 tokens (   14.29 ms per token,    69.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.99 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3786.07 ms /   114 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1262.86 ms /    90 tokens (   14.03 ms per token,    71.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.11 ms /    15 runs   (  157.94 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3637.38 ms /   105 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1267.41 ms /    91 tokens (   13.93 ms per token,    71.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.99 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3639.64 ms /   106 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1296.71 ms /    91 tokens (   14.25 ms per token,    70.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.66 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3668.73 ms /   106 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1200.06 ms /    92 tokens (   13.04 ms per token,    76.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.61 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3572.95 ms /   107 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1287.91 ms /    90 tokens (   14.31 ms per token,    69.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.25 ms /    15 runs   (  157.82 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3660.40 ms /   105 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1320.23 ms /   100 tokens (   13.20 ms per token,    75.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.96 ms /    15 runs   (  157.66 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3690.64 ms /   115 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1261.32 ms /    96 tokens (   13.14 ms per token,    76.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.11 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3634.78 ms /   111 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 73 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1360.02 ms /    98 tokens (   13.88 ms per token,    72.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.55 ms /    15 runs   (  158.17 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3738.19 ms /   113 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1195.46 ms /    86 tokens (   13.90 ms per token,    71.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.36 ms /    15 runs   (  157.96 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3570.14 ms /   101 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1445.46 ms /   102 tokens (   14.17 ms per token,    70.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.54 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3817.62 ms /   117 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1558.35 ms /    75 tokens (   20.78 ms per token,    48.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.95 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3929.38 ms /    90 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1409.91 ms /   101 tokens (   13.96 ms per token,    71.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.20 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3782.76 ms /   116 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1336.40 ms /    93 tokens (   14.37 ms per token,    69.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.45 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3709.22 ms /   108 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 70 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1204.87 ms /    87 tokens (   13.85 ms per token,    72.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.40 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3576.40 ms /   102 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1783.57 ms /   127 tokens (   14.04 ms per token,    71.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.64 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4156.32 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1663.23 ms /   121 tokens (   13.75 ms per token,    72.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.01 ms /    15 runs   (  158.07 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4040.03 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1672.27 ms /   128 tokens (   13.06 ms per token,    76.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.68 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4043.75 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1717.86 ms /   125 tokens (   13.74 ms per token,    72.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.52 ms /    15 runs   (  158.17 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4096.23 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1714.01 ms /   125 tokens (   13.71 ms per token,    72.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.58 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4086.46 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 74 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1754.27 ms /   132 tokens (   13.29 ms per token,    75.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2403.93 ms /    15 runs   (  160.26 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4164.22 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1689.51 ms /   123 tokens (   13.74 ms per token,    72.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.59 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4060.87 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1629.93 ms /   124 tokens (   13.14 ms per token,    76.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.14 ms /    15 runs   (  158.01 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4005.82 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1617.50 ms /   124 tokens (   13.04 ms per token,    76.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.90 ms /    15 runs   (  158.06 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3994.09 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1723.88 ms /   125 tokens (   13.79 ms per token,    72.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.08 ms /    15 runs   (  157.94 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4098.59 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1675.68 ms /   123 tokens (   13.62 ms per token,    73.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.34 ms /    15 runs   (  158.62 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    4060.72 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1816.98 ms /   133 tokens (   13.66 ms per token,    73.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.03 ms /    15 runs   (  158.40 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4198.85 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1766.10 ms /   129 tokens (   13.69 ms per token,    73.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.47 ms /    15 runs   (  157.90 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4140.32 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 73 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1791.69 ms /   131 tokens (   13.68 ms per token,    73.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.92 ms /    15 runs   (  157.99 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4167.36 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1673.66 ms /   119 tokens (   14.06 ms per token,    71.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.83 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4045.28 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1830.69 ms /   135 tokens (   13.56 ms per token,    73.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.87 ms /    15 runs   (  157.99 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4206.26 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1445.06 ms /   108 tokens (   13.38 ms per token,    74.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.47 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3816.08 ms /   123 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1875.58 ms /   134 tokens (   14.00 ms per token,    71.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.34 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4251.71 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1723.55 ms /   126 tokens (   13.68 ms per token,    73.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.39 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4094.89 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 70 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1597.99 ms /   120 tokens (   13.32 ms per token,    75.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.92 ms /    15 runs   (  158.39 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3979.73 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1843.52 ms /   131 tokens (   14.07 ms per token,    71.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.26 ms /    15 runs   (  157.68 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4214.59 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1748.41 ms /   125 tokens (   13.99 ms per token,    71.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.51 ms /    15 runs   (  158.63 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    4133.85 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1772.79 ms /   132 tokens (   13.43 ms per token,    74.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.35 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4143.96 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1794.37 ms /   129 tokens (   13.91 ms per token,    71.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.46 ms /    15 runs   (  158.36 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4175.64 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1759.48 ms /   129 tokens (   13.64 ms per token,    73.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.12 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4132.36 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 74 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1820.39 ms /   136 tokens (   13.39 ms per token,    74.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.30 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4196.51 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 50 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1773.51 ms /   127 tokens (   13.96 ms per token,    71.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.04 ms /    15 runs   (  157.74 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4145.28 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1678.67 ms /   128 tokens (   13.11 ms per token,    76.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.17 ms /    15 runs   (  157.94 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4053.57 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1720.01 ms /   128 tokens (   13.44 ms per token,    74.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.89 ms /    15 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4093.79 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1803.34 ms /   129 tokens (   13.98 ms per token,    71.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.62 ms /    15 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4179.68 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1727.91 ms /   127 tokens (   13.61 ms per token,    73.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.17 ms /    15 runs   (  157.68 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4098.82 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1868.05 ms /   137 tokens (   13.64 ms per token,    73.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.84 ms /    15 runs   (  157.99 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4243.54 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1915.33 ms /   133 tokens (   14.40 ms per token,    69.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.81 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4286.91 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 73 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1851.75 ms /   135 tokens (   13.72 ms per token,    72.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.50 ms /    15 runs   (  158.50 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4235.10 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1684.61 ms /   123 tokens (   13.70 ms per token,    73.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.89 ms /    15 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4058.27 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1917.81 ms /   139 tokens (   13.80 ms per token,    72.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.53 ms /    15 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4294.23 ms /   154 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1459.74 ms /   112 tokens (   13.03 ms per token,    76.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.49 ms /    15 runs   (  158.17 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3837.95 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1876.13 ms /   138 tokens (   13.60 ms per token,    73.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.62 ms /    15 runs   (  157.97 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4251.58 ms /   153 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1823.84 ms /   130 tokens (   14.03 ms per token,    71.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.26 ms /    15 runs   (  157.82 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4196.87 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 70 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1654.83 ms /   124 tokens (   13.35 ms per token,    74.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.46 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4031.20 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1826.65 ms /   134 tokens (   13.63 ms per token,    73.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.53 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4198.95 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1722.03 ms /   128 tokens (   13.45 ms per token,    74.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.83 ms /    15 runs   (  158.12 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4099.68 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1871.77 ms /   135 tokens (   13.86 ms per token,    72.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.02 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4245.62 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 72 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1729.79 ms /   132 tokens (   13.10 ms per token,    76.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.91 ms /    15 runs   (  157.99 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4105.47 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1774.73 ms /   132 tokens (   13.44 ms per token,    74.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.47 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4148.05 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 74 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1926.92 ms /   139 tokens (   13.86 ms per token,    72.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.38 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4303.13 ms /   154 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 50 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1770.56 ms /   130 tokens (   13.62 ms per token,    73.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.37 ms /    15 runs   (  157.82 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4143.68 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 72 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1789.34 ms /   131 tokens (   13.66 ms per token,    73.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.52 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4165.65 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1787.33 ms /   131 tokens (   13.64 ms per token,    73.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.63 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4159.48 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1760.76 ms /   132 tokens (   13.34 ms per token,    74.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.88 ms /    15 runs   (  157.99 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4136.42 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1783.07 ms /   130 tokens (   13.72 ms per token,    72.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.12 ms /    15 runs   (  157.67 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4154.01 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1825.01 ms /   140 tokens (   13.04 ms per token,    76.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.93 ms /    15 runs   (  158.00 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4200.91 ms /   155 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1864.08 ms /   136 tokens (   13.71 ms per token,    72.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2419.56 ms /    15 runs   (  161.30 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    4289.65 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 73 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1872.92 ms /   138 tokens (   13.57 ms per token,    73.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.81 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4249.53 ms /   153 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1757.26 ms /   126 tokens (   13.95 ms per token,    71.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.11 ms /    15 runs   (  157.74 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4129.13 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1938.72 ms /   142 tokens (   13.65 ms per token,    73.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.50 ms /    15 runs   (  158.43 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4321.09 ms /   157 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1573.13 ms /   115 tokens (   13.68 ms per token,    73.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.21 ms /    15 runs   (  157.68 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3943.85 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1950.75 ms /   141 tokens (   13.84 ms per token,    72.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.47 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4326.95 ms /   156 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1844.84 ms /   133 tokens (   13.87 ms per token,    72.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.63 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4217.06 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 70 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1759.09 ms /   127 tokens (   13.85 ms per token,    72.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.15 ms /    15 runs   (  158.14 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4137.02 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1830.15 ms /   140 tokens (   13.07 ms per token,    76.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.68 ms /    15 runs   (  158.58 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4214.71 ms /   155 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1868.55 ms /   134 tokens (   13.94 ms per token,    71.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.27 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4244.63 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1970.63 ms /   141 tokens (   13.98 ms per token,    71.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.87 ms /    15 runs   (  158.06 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4347.44 ms /   156 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 72 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1907.06 ms /   138 tokens (   13.82 ms per token,    72.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.76 ms /    15 runs   (  158.98 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    4297.73 ms /   153 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 50 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1872.86 ms /   138 tokens (   13.57 ms per token,    73.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.98 ms /    15 runs   (  157.93 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4247.73 ms /   153 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 74 prefix-match hit, remaining 145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2038.12 ms /   145 tokens (   14.06 ms per token,    71.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.81 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4414.91 ms /   160 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 50 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1805.40 ms /   136 tokens (   13.28 ms per token,    75.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.64 ms /    15 runs   (  157.91 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4179.86 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 72 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1864.94 ms /   137 tokens (   13.61 ms per token,    73.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.99 ms /    15 runs   (  158.00 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4240.66 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1877.54 ms /   137 tokens (   13.70 ms per token,    72.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.70 ms /    15 runs   (  157.98 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4253.14 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 68 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1917.87 ms /   138 tokens (   13.90 ms per token,    71.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.83 ms /    15 runs   (  158.06 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4294.54 ms /   153 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1775.43 ms /   136 tokens (   13.05 ms per token,    76.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.60 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4147.84 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 67 prefix-match hit, remaining 146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2014.86 ms /   146 tokens (   13.80 ms per token,    72.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.77 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4391.60 ms /   161 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1924.91 ms /   142 tokens (   13.56 ms per token,    73.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.21 ms /    15 runs   (  158.35 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4305.84 ms /   157 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 73 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1883.19 ms /   144 tokens (   13.08 ms per token,    76.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.50 ms /    15 runs   (  158.23 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4262.57 ms /   159 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1715.37 ms /   132 tokens (   13.00 ms per token,    76.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.89 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4086.99 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1983.00 ms /   148 tokens (   13.40 ms per token,    74.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2374.03 ms /    15 runs   (  158.27 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4363.06 ms /   163 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 50 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1658.21 ms /   121 tokens (   13.70 ms per token,    72.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.75 ms /    15 runs   (  158.12 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4035.84 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 57 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1983.37 ms /   147 tokens (   13.49 ms per token,    74.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2412.95 ms /    15 runs   (  160.86 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    4402.36 ms /   162 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1910.73 ms /   139 tokens (   13.75 ms per token,    72.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.11 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4283.69 ms /   154 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 70 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1851.85 ms /   133 tokens (   13.92 ms per token,    71.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.20 ms /    15 runs   (  157.95 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4226.81 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1616.55 ms /   124 tokens (   13.04 ms per token,    76.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.23 ms /    15 runs   (  157.68 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3987.67 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1663.74 ms /   118 tokens (   14.10 ms per token,    70.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.83 ms /    15 runs   (  157.92 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4038.04 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1757.05 ms /   125 tokens (   14.06 ms per token,    71.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.38 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4129.33 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1667.85 ms /   122 tokens (   13.67 ms per token,    73.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.60 ms /    15 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4044.20 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1745.14 ms /   122 tokens (   14.30 ms per token,    69.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.10 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4118.90 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 74 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1768.01 ms /   129 tokens (   13.71 ms per token,    72.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.93 ms /    15 runs   (  158.40 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4149.72 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1575.78 ms /   120 tokens (   13.13 ms per token,    76.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.94 ms /    15 runs   (  157.66 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3946.46 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 72 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1666.65 ms /   121 tokens (   13.77 ms per token,    72.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2426.89 ms /    15 runs   (  161.79 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    4099.59 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1655.45 ms /   121 tokens (   13.68 ms per token,    73.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.76 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4027.01 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1757.18 ms /   122 tokens (   14.40 ms per token,    69.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.92 ms /    15 runs   (  158.66 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    4142.76 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1582.88 ms /   120 tokens (   13.19 ms per token,    75.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.47 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3955.13 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 67 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1815.56 ms /   130 tokens (   13.97 ms per token,    71.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.49 ms /    15 runs   (  157.97 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4190.83 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1760.25 ms /   126 tokens (   13.97 ms per token,    71.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.05 ms /    15 runs   (  157.74 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4132.12 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 73 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1672.24 ms /   128 tokens (   13.06 ms per token,    76.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.65 ms /    15 runs   (  157.98 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4047.71 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1554.44 ms /   116 tokens (   13.40 ms per token,    74.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.33 ms /    15 runs   (  157.82 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3927.46 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1759.33 ms /   132 tokens (   13.33 ms per token,    75.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.27 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4135.31 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 50 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1476.33 ms /   105 tokens (   14.06 ms per token,    71.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.71 ms /    15 runs   (  157.91 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3850.68 ms /   120 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 57 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1814.45 ms /   131 tokens (   13.85 ms per token,    72.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.85 ms /    15 runs   (  157.92 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4189.11 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1699.08 ms /   123 tokens (   13.81 ms per token,    72.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.20 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4071.05 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 70 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1613.23 ms /   117 tokens (   13.79 ms per token,    72.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.80 ms /    15 runs   (  157.92 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3987.84 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1314.82 ms /    94 tokens (   13.99 ms per token,    71.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.62 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3687.91 ms /   109 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1092.47 ms /    77 tokens (   14.19 ms per token,    70.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.35 ms /    15 runs   (  158.16 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3469.88 ms /    92 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1321.40 ms /    91 tokens (   14.52 ms per token,    68.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.69 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3694.41 ms /   106 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1052.58 ms /    75 tokens (   14.03 ms per token,    71.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2374.14 ms /    15 runs   (  158.28 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3431.75 ms /    90 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1397.26 ms /   101 tokens (   13.83 ms per token,    72.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.54 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3770.28 ms /   116 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 78 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1049.54 ms /    75 tokens (   13.99 ms per token,    71.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.83 ms /    15 runs   (  157.92 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3423.42 ms /    90 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1273.93 ms /    91 tokens (   14.00 ms per token,    71.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.73 ms /    15 runs   (  158.12 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3650.90 ms /   106 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1153.96 ms /    83 tokens (   13.90 ms per token,    71.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.05 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3527.16 ms /    98 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1238.43 ms /    85 tokens (   14.57 ms per token,    68.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.05 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3611.72 ms /   100 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1085.49 ms /    77 tokens (   14.10 ms per token,    70.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.80 ms /    15 runs   (  158.25 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3464.40 ms /    92 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1253.29 ms /    96 tokens (   13.06 ms per token,    76.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.80 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3624.43 ms /   111 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 74 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1135.07 ms /    77 tokens (   14.74 ms per token,    67.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.37 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3510.54 ms /    92 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1304.23 ms /    94 tokens (   13.87 ms per token,    72.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.69 ms /    15 runs   (  158.18 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3682.23 ms /   109 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1042.56 ms /    74 tokens (   14.09 ms per token,    70.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.46 ms /    15 runs   (  157.90 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3416.06 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1304.57 ms /    91 tokens (   14.34 ms per token,    69.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.76 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3675.59 ms /   106 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1040.77 ms /    74 tokens (   14.06 ms per token,    71.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.83 ms /    15 runs   (  158.26 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3419.65 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1415.26 ms /   102 tokens (   13.88 ms per token,    72.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.38 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3785.99 ms /   117 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 80 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1069.88 ms /    74 tokens (   14.46 ms per token,    69.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.96 ms /    15 runs   (  158.00 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3444.94 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1242.39 ms /    89 tokens (   13.96 ms per token,    71.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.60 ms /    15 runs   (  158.24 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3621.45 ms /   104 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 75 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1042.80 ms /    74 tokens (   14.09 ms per token,    70.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2384.40 ms /    15 runs   (  158.96 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    3432.29 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1304.23 ms /    93 tokens (   14.02 ms per token,    71.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.48 ms /    15 runs   (  157.90 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3678.19 ms /   108 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 70 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1058.07 ms /    74 tokens (   14.30 ms per token,    69.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2374.56 ms /    15 runs   (  158.30 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3437.73 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1572.64 ms /   115 tokens (   13.68 ms per token,    73.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.50 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3943.81 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 93 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1081.50 ms /    74 tokens (   14.61 ms per token,    68.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.76 ms /    15 runs   (  158.52 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3464.47 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1372.31 ms /    90 tokens (   15.25 ms per token,    65.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.81 ms /    15 runs   (  158.19 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3750.54 ms /   105 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1040.13 ms /    74 tokens (   14.06 ms per token,    71.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.28 ms /    15 runs   (  157.95 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3414.45 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1409.58 ms /   101 tokens (   13.96 ms per token,    71.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.00 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3783.10 ms /   116 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 79 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1054.14 ms /    74 tokens (   14.25 ms per token,    70.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.88 ms /    15 runs   (  158.26 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3433.13 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1729.88 ms /   127 tokens (   13.62 ms per token,    73.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.03 ms /    15 runs   (  157.74 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4101.60 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1511.64 ms /   110 tokens (   13.74 ms per token,    72.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.41 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3884.63 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1622.24 ms /   124 tokens (   13.08 ms per token,    76.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.28 ms /    15 runs   (  158.09 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3999.22 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1416.80 ms /   108 tokens (   13.12 ms per token,    76.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.49 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3792.82 ms /   123 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1835.60 ms /   134 tokens (   13.70 ms per token,    73.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.13 ms /    15 runs   (  157.88 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4209.54 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 78 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1463.60 ms /   108 tokens (   13.55 ms per token,    73.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.74 ms /    15 runs   (  157.92 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3837.94 ms /   123 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1620.85 ms /   124 tokens (   13.07 ms per token,    76.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.53 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3994.20 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1561.10 ms /   116 tokens (   13.46 ms per token,    74.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.08 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3934.89 ms /   131 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1656.49 ms /   118 tokens (   14.04 ms per token,    71.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.63 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4028.89 ms /   133 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1515.44 ms /   110 tokens (   13.78 ms per token,    72.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2406.42 ms /    15 runs   (  160.43 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3927.57 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1789.52 ms /   129 tokens (   13.87 ms per token,    72.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.54 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4161.80 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 74 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1572.39 ms /   110 tokens (   14.29 ms per token,    69.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.79 ms /    15 runs   (  157.92 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3946.86 ms /   125 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1726.14 ms /   127 tokens (   13.59 ms per token,    73.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.48 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4098.15 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1522.12 ms /   107 tokens (   14.23 ms per token,    70.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.05 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3894.58 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1617.11 ms /   124 tokens (   13.04 ms per token,    76.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.38 ms /    15 runs   (  158.09 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3994.29 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1469.18 ms /   107 tokens (   13.73 ms per token,    72.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.92 ms /    15 runs   (  157.66 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3839.40 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1870.07 ms /   135 tokens (   13.85 ms per token,    72.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.15 ms /    15 runs   (  157.94 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4245.05 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 80 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1484.13 ms /   107 tokens (   13.87 ms per token,    72.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2375.03 ms /    15 runs   (  158.34 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3864.64 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1670.84 ms /   122 tokens (   13.70 ms per token,    73.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.68 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4044.25 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 75 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1465.49 ms /   107 tokens (   13.70 ms per token,    73.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.73 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3837.73 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1719.61 ms /   126 tokens (   13.65 ms per token,    73.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.84 ms /    15 runs   (  158.66 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    4105.12 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 70 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1468.40 ms /   107 tokens (   13.72 ms per token,    72.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.73 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3839.59 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1965.09 ms /   148 tokens (   13.28 ms per token,    75.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.44 ms /    15 runs   (  157.96 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4340.48 ms /   163 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 93 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1494.14 ms /   107 tokens (   13.96 ms per token,    71.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2377.36 ms /    15 runs   (  158.49 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3876.97 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1675.04 ms /   123 tokens (   13.62 ms per token,    73.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.29 ms /    15 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4047.14 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1466.31 ms /   107 tokens (   13.70 ms per token,    72.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.17 ms /    15 runs   (  157.74 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3837.95 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1823.89 ms /   134 tokens (   13.61 ms per token,    73.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.35 ms /    15 runs   (  158.16 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4201.98 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 79 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1467.01 ms /   107 tokens (   13.71 ms per token,    72.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.58 ms /    15 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3842.92 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1813.59 ms /   131 tokens (   13.84 ms per token,    72.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.96 ms /    15 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4187.24 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1609.02 ms /   114 tokens (   14.11 ms per token,    70.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.31 ms /    15 runs   (  157.89 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3983.02 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1675.02 ms /   128 tokens (   13.09 ms per token,    76.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.02 ms /    15 runs   (  157.60 ms per token,     6.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    4044.36 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1512.32 ms /   112 tokens (   13.50 ms per token,    74.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.33 ms /    15 runs   (  157.82 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3885.20 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1915.60 ms /   138 tokens (   13.88 ms per token,    72.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.02 ms /    15 runs   (  158.07 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4292.47 ms /   153 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 78 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1926.27 ms /   112 tokens (   17.20 ms per token,    58.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.50 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4302.43 ms /   127 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1694.18 ms /   128 tokens (   13.24 ms per token,    75.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.21 ms /    15 runs   (  158.61 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    4079.31 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1570.42 ms /   120 tokens (   13.09 ms per token,    76.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.74 ms /    15 runs   (  157.98 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3945.93 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1669.73 ms /   122 tokens (   13.69 ms per token,    73.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.29 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4040.80 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1599.77 ms /   114 tokens (   14.03 ms per token,    71.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.57 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3972.02 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1821.45 ms /   133 tokens (   13.70 ms per token,    73.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.89 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4193.17 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 74 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1563.41 ms /   114 tokens (   13.71 ms per token,    72.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.10 ms /    15 runs   (  157.94 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3938.16 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1809.59 ms /   131 tokens (   13.81 ms per token,    72.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.01 ms /    15 runs   (  158.13 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4187.43 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1517.26 ms /   111 tokens (   13.67 ms per token,    73.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.30 ms /    15 runs   (  157.95 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3891.79 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1697.86 ms /   128 tokens (   13.26 ms per token,    75.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.96 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4069.58 ms /   143 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1533.39 ms /   111 tokens (   13.81 ms per token,    72.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.10 ms /    15 runs   (  158.07 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3909.66 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1875.77 ms /   139 tokens (   13.49 ms per token,    74.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.36 ms /    15 runs   (  157.82 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4248.86 ms /   154 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 80 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1551.91 ms /   111 tokens (   13.98 ms per token,    71.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.31 ms /    15 runs   (  157.95 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3926.89 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1755.98 ms /   126 tokens (   13.94 ms per token,    71.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.07 ms /    15 runs   (  158.40 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4137.91 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 75 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1519.30 ms /   111 tokens (   13.69 ms per token,    73.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.47 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3895.39 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1807.29 ms /   130 tokens (   13.90 ms per token,    71.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.10 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4181.17 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 70 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1554.26 ms /   111 tokens (   14.00 ms per token,    71.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.13 ms /    15 runs   (  157.88 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3928.01 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1989.70 ms /   152 tokens (   13.09 ms per token,    76.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.17 ms /    15 runs   (  158.01 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4365.91 ms /   167 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 93 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1556.59 ms /   111 tokens (   14.02 ms per token,    71.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.06 ms /    15 runs   (  158.14 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3934.02 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1777.29 ms /   127 tokens (   13.99 ms per token,    71.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.61 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4148.72 ms /   142 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1526.97 ms /   111 tokens (   13.76 ms per token,    72.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.38 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3898.93 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1872.50 ms /   138 tokens (   13.57 ms per token,    73.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.63 ms /    15 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4246.01 ms /   153 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 79 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1559.09 ms /   111 tokens (   14.05 ms per token,    71.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.32 ms /    15 runs   (  158.22 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3938.09 ms /   126 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1819.40 ms /   134 tokens (   13.58 ms per token,    73.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.72 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4191.80 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1651.28 ms /   117 tokens (   14.11 ms per token,    70.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.99 ms /    15 runs   (  158.00 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4027.08 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1828.76 ms /   131 tokens (   13.96 ms per token,    71.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.16 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4201.77 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1589.02 ms /   115 tokens (   13.82 ms per token,    72.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.52 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3962.11 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1918.06 ms /   141 tokens (   13.60 ms per token,    73.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.03 ms /    15 runs   (  158.14 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4295.94 ms /   156 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 78 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1629.31 ms /   115 tokens (   14.17 ms per token,    70.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.10 ms /    15 runs   (  158.01 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4005.11 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1779.85 ms /   131 tokens (   13.59 ms per token,    73.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.50 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4153.06 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1676.29 ms /   123 tokens (   13.63 ms per token,    73.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.54 ms /    15 runs   (  158.44 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4058.47 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1708.37 ms /   125 tokens (   13.67 ms per token,    73.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.98 ms /    15 runs   (  157.67 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4079.12 ms /   140 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1608.61 ms /   117 tokens (   13.75 ms per token,    72.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.23 ms /    15 runs   (  158.22 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3987.67 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1809.56 ms /   136 tokens (   13.31 ms per token,    75.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2415.78 ms /    15 runs   (  161.05 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    4231.32 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 74 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1641.33 ms /   117 tokens (   14.03 ms per token,    71.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.97 ms /    15 runs   (  158.06 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4018.08 ms /   132 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1830.16 ms /   134 tokens (   13.66 ms per token,    73.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.07 ms /    15 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4203.12 ms /   149 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1616.94 ms /   114 tokens (   14.18 ms per token,    70.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.27 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3993.00 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1819.35 ms /   131 tokens (   13.89 ms per token,    72.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.62 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4190.77 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1587.98 ms /   114 tokens (   13.93 ms per token,    71.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.89 ms /    15 runs   (  157.79 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3960.53 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1938.67 ms /   142 tokens (   13.65 ms per token,    73.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.65 ms /    15 runs   (  157.91 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4313.12 ms /   157 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 80 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1631.85 ms /   114 tokens (   14.31 ms per token,    69.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.19 ms /    15 runs   (  158.15 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4009.74 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 54 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1761.99 ms /   129 tokens (   13.66 ms per token,    73.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.20 ms /    15 runs   (  157.95 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4136.68 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 75 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1608.86 ms /   114 tokens (   14.11 ms per token,    70.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.34 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3984.86 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1849.56 ms /   133 tokens (   13.91 ms per token,    71.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.78 ms /    15 runs   (  157.92 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4223.99 ms /   148 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 70 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1569.43 ms /   114 tokens (   13.77 ms per token,    72.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.41 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3945.54 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2152.71 ms /   155 tokens (   13.89 ms per token,    72.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.29 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4529.05 ms /   170 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 93 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1575.62 ms /   114 tokens (   13.82 ms per token,    72.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.59 ms /    15 runs   (  157.97 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3950.92 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1778.37 ms /   130 tokens (   13.68 ms per token,    73.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.36 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4149.53 ms /   145 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1592.53 ms /   114 tokens (   13.97 ms per token,    71.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.14 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3965.38 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1912.68 ms /   141 tokens (   13.57 ms per token,    73.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2374.99 ms /    15 runs   (  158.33 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4293.60 ms /   156 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 79 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1596.47 ms /   114 tokens (   14.00 ms per token,    71.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.80 ms /    15 runs   (  158.12 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3973.90 ms /   129 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1829.18 ms /   140 tokens (   13.07 ms per token,    76.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.46 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4202.52 ms /   155 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1707.85 ms /   123 tokens (   13.88 ms per token,    72.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.50 ms /    15 runs   (  158.17 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4086.07 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1902.15 ms /   137 tokens (   13.88 ms per token,    72.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.10 ms /    15 runs   (  157.87 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4276.13 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 68 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1706.61 ms /   121 tokens (   14.10 ms per token,    70.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2379.06 ms /    15 runs   (  158.60 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4091.35 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1991.65 ms /   147 tokens (   13.55 ms per token,    73.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.53 ms /    15 runs   (  157.97 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4367.11 ms /   162 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 78 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1706.70 ms /   121 tokens (   14.10 ms per token,    70.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.85 ms /    15 runs   (  157.99 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4082.32 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1886.44 ms /   137 tokens (   13.77 ms per token,    72.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.31 ms /    15 runs   (  157.89 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4260.60 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 68 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1808.95 ms /   129 tokens (   14.02 ms per token,    71.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.33 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4185.09 ms /   144 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1779.73 ms /   131 tokens (   13.59 ms per token,    73.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.99 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4151.52 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1712.76 ms /   123 tokens (   13.92 ms per token,    71.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.43 ms /    15 runs   (  157.96 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4087.94 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1964.34 ms /   142 tokens (   13.83 ms per token,    72.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.82 ms /    15 runs   (  157.99 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4340.09 ms /   157 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 74 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1681.23 ms /   123 tokens (   13.67 ms per token,    73.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.60 ms /    15 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4058.50 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1854.23 ms /   140 tokens (   13.24 ms per token,    75.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.31 ms /    15 runs   (  158.09 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4231.41 ms /   155 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 71 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1573.50 ms /   120 tokens (   13.11 ms per token,    76.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.53 ms /    15 runs   (  158.44 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3955.80 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1865.78 ms /   137 tokens (   13.62 ms per token,    73.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.78 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4239.44 ms /   152 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 68 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1613.94 ms /   120 tokens (   13.45 ms per token,    74.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.17 ms /    15 runs   (  157.94 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3988.86 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1935.21 ms /   148 tokens (   13.08 ms per token,    76.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2376.36 ms /    15 runs   (  158.42 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4317.42 ms /   163 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 80 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1592.70 ms /   120 tokens (   13.27 ms per token,    75.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.64 ms /    15 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3970.13 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 54 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1883.06 ms /   135 tokens (   13.95 ms per token,    71.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.91 ms /    15 runs   (  157.93 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4257.76 ms /   150 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 75 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1573.55 ms /   120 tokens (   13.11 ms per token,    76.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2391.11 ms /    15 runs   (  159.41 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3970.45 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1998.06 ms /   139 tokens (   14.37 ms per token,    69.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.96 ms /    15 runs   (  157.93 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4372.68 ms /   154 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 70 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1590.15 ms /   120 tokens (   13.25 ms per token,    75.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.23 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3966.09 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2239.24 ms /   161 tokens (   13.91 ms per token,    71.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.65 ms /    15 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4616.13 ms /   176 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 93 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1571.29 ms /   120 tokens (   13.09 ms per token,    76.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2387.87 ms /    15 runs   (  159.19 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    3965.04 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1782.77 ms /   136 tokens (   13.11 ms per token,    76.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.55 ms /    15 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4154.99 ms /   151 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 68 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1579.39 ms /   120 tokens (   13.16 ms per token,    75.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.49 ms /    15 runs   (  157.97 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3954.67 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2019.83 ms /   147 tokens (   13.74 ms per token,    72.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2374.15 ms /    15 runs   (  158.28 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4400.10 ms /   162 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 79 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1616.98 ms /   120 tokens (   13.47 ms per token,    74.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.85 ms /    15 runs   (  158.26 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3996.54 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1663.31 ms /   124 tokens (   13.41 ms per token,    74.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.30 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4034.44 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1472.42 ms /   107 tokens (   13.76 ms per token,    72.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.38 ms /    15 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3843.37 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1673.76 ms /   121 tokens (   13.83 ms per token,    72.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.76 ms /    15 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4046.37 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1502.45 ms /   105 tokens (   14.31 ms per token,    69.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2369.20 ms /    15 runs   (  157.95 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3877.34 ms /   120 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1807.06 ms /   131 tokens (   13.79 ms per token,    72.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.79 ms /    15 runs   (  157.79 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4179.74 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 78 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1455.08 ms /   105 tokens (   13.86 ms per token,    72.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.25 ms /    15 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3830.94 ms /   120 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1658.23 ms /   121 tokens (   13.70 ms per token,    72.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.53 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4029.67 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1633.70 ms /   113 tokens (   14.46 ms per token,    69.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.68 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4007.13 ms /   128 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 46 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1622.49 ms /   115 tokens (   14.11 ms per token,    70.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.52 ms /    15 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3993.62 ms /   130 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 63 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1504.50 ms /   107 tokens (   14.06 ms per token,    71.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.15 ms /    15 runs   (  157.74 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3876.06 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1718.37 ms /   126 tokens (   13.64 ms per token,    73.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.74 ms /    15 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4094.77 ms /   141 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 74 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1477.27 ms /   107 tokens (   13.81 ms per token,    72.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.18 ms /    15 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3850.03 ms /   122 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1650.42 ms /   124 tokens (   13.31 ms per token,    75.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.47 ms /    15 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4023.66 ms /   139 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 71 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1400.97 ms /   104 tokens (   13.47 ms per token,    74.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.63 ms /    15 runs   (  157.71 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3772.11 ms /   119 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 45 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1703.74 ms /   121 tokens (   14.08 ms per token,    71.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.97 ms /    15 runs   (  157.66 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4074.44 ms /   136 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1402.68 ms /   104 tokens (   13.49 ms per token,    74.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2367.79 ms /    15 runs   (  157.85 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3775.95 ms /   119 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1724.34 ms /   132 tokens (   13.06 ms per token,    76.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.04 ms /    15 runs   (  158.14 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4102.18 ms /   147 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 80 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1367.44 ms /   104 tokens (   13.15 ms per token,    76.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.67 ms /    15 runs   (  158.24 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3746.80 ms /   119 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 54 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2243.61 ms /   119 tokens (   18.85 ms per token,    53.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.79 ms /    15 runs   (  157.72 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4615.09 ms /   134 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 75 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1363.22 ms /   104 tokens (   13.11 ms per token,    76.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.21 ms /    15 runs   (  158.21 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3742.00 ms /   119 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 45 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1676.05 ms /   123 tokens (   13.63 ms per token,    73.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2372.25 ms /    15 runs   (  158.15 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4054.07 ms /   138 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 70 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1369.25 ms /   104 tokens (   13.17 ms per token,    75.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2371.65 ms /    15 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3746.45 ms /   119 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2014.95 ms /   145 tokens (   13.90 ms per token,    71.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2370.41 ms /    15 runs   (  158.03 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4391.12 ms /   160 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Llama.generate: 93 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1392.36 ms /   104 tokens (   13.39 ms per token,    74.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2385.66 ms /    15 runs   (  159.04 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    3783.70 ms /   119 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1695.52 ms /   120 tokens (   14.13 ms per token,    70.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.47 ms /    15 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4067.77 ms /   135 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 68 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1379.88 ms /   104 tokens (   13.27 ms per token,    75.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2366.02 ms /    15 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3751.15 ms /   119 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 46 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    2035.99 ms /   131 tokens (   15.54 ms per token,    64.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.87 ms /    15 runs   (  157.66 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4406.58 ms /   146 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 79 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1847.54 ms\n",
      "llama_perf_context_print: prompt eval time =    1491.25 ms /   104 tokens (   14.34 ms per token,    69.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.59 ms /    15 runs   (  157.91 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3865.37 ms /   119 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "criteria_ids = [\n",
    "    \"harm\",\n",
    "    \"social_bias\",\n",
    "    \"profanity\",\n",
    "    \"sexual_content\",\n",
    "    \"unethical_behavior\",\n",
    "    \"violence\",\n",
    "]\n",
    "for idx, df in enumerate(dfs):\n",
    "    for criterion in criteria_ids:\n",
    "        guardian_config = {\"criteria_id\": criterion}\n",
    "        col_q = []\n",
    "        col_qa = []\n",
    "        for _, row in df.iterrows(): \n",
    "            user_text = row['question']\n",
    "            response_text = row['answer']\n",
    "\n",
    "            messages = [{\"role\": \"user\", \"content\": user_text}]\n",
    "\n",
    "            prompt = tokenizer.apply_chat_template(messages, guardian_config = guardian_config, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "            output = llm(\n",
    "                prompt,\n",
    "                max_tokens=32,\n",
    "                temperature=0.0,\n",
    "                logprobs=20,\n",
    "                echo=False\n",
    "            )\n",
    "\n",
    "            response = output[\"choices\"][0][\"text\"]\n",
    "\n",
    "            score_q, _ = parse_response(response)\n",
    "\n",
    "            col_q.append(score_q)\n",
    "\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "            prompt = tokenizer.apply_chat_template(messages, guardian_config = guardian_config, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "            output = llm(\n",
    "                prompt,\n",
    "                max_tokens=32,\n",
    "                temperature=0.0,\n",
    "                logprobs=20,\n",
    "                echo=False\n",
    "            )\n",
    "\n",
    "            response = output[\"choices\"][0][\"text\"]\n",
    "\n",
    "            score_qa, _ = parse_response(response)\n",
    "\n",
    "            col_qa.append(score_qa)\n",
    "        \n",
    "        # Add new columns\n",
    "        df[f\"{criterion}_q\"] = col_q\n",
    "        df[f\"{criterion}_qa\"] = col_qa\n",
    "    out_path = os.path.join(\"../data/out\", csvs[idx])\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    df.to_csv(out_path, index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 3 Use Case - Toggle Reasoning\n",
    "\n",
    "For complex use-case that require some reasoning, invoke thinking mode.\n",
    "\n",
    "<details>\n",
    "    <summary>Code Hint</summary>\n",
    "    To enable thinking mode, pass <b><i>'think=True'</i></b> argument through <b>'apply_chat_template' method</b>.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89023481-9775-4781-a9b7-568e6a64d907",
   "metadata": {},
   "source": [
    "#### Grounded hallucination detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6a7a5-7218-4699-8d63-88735a48bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\"\"Eat (1964) is a 45-minute underground film created by Andy Warhol and featuring painter Robert Indiana, filmed on Sunday, February 2, 1964, in Indiana's studio. The film was first shown by Jonas Mekas on July 16, 1964, at the Washington Square Gallery at 530 West Broadway.\n",
    "Jonas Mekas (December 24, 1922 â€“ January 23, 2019) was a Lithuanian-American filmmaker, poet, and artist who has been called \"the godfather of American avant-garde cinema\". Mekas's work has been exhibited in museums and at festivals worldwide.\"\"\"\n",
    "documents = [{'doc_id':'0', 'text': context_text}]\n",
    "response_text = \"The film Eat was first shown by Jonas Mekas on December 24, 1922 at the Washington Square Gallery at 530 West Broadway.\"\n",
    "\n",
    "messages = [{\"role\": \"assistant\", \"content\": response_text}]\n",
    "\n",
    "guardian_config = {\"criteria_id\": \"groundedness\"}\n",
    "chat = tokenizer.apply_chat_template(messages, guardian_config = guardian_config, documents=documents, think=True, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "output = model.generate(chat, sampling_params, use_tqdm=False)\n",
    "response = output[0].outputs[0].text.strip()\n",
    "\n",
    "score, trace = parse_response(response)\n",
    "\n",
    "print(f\"# score: {score}\\n\")\n",
    "print(f\"# trace: {trace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420166b0-8f59-4f54-b069-c7ebb2b3f7ef",
   "metadata": {},
   "source": [
    "#### Function call hallucination detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eccdab8-b159-40f5-84cd-4854d359a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "  {\n",
    "    \"name\": \"comment_list\",\n",
    "    \"description\": \"Fetches a list of comments for a specified IBM video using the given API.\",\n",
    "    \"parameters\": {\n",
    "      \"aweme_id\": {\n",
    "        \"description\": \"The ID of the IBM video.\",\n",
    "        \"type\": \"int\",\n",
    "        \"default\": \"7178094165614464282\"\n",
    "      },\n",
    "      \"cursor\": {\n",
    "        \"description\": \"The cursor for pagination to get the next page of comments. Defaults to 0.\",\n",
    "        \"type\": \"int, optional\",\n",
    "        \"default\": \"0\"\n",
    "      },\n",
    "      \"count\": {\n",
    "        \"description\": \"The number of comments to fetch. Maximum is 30. Defaults to 20.\",\n",
    "        \"type\": \"int, optional\",\n",
    "        \"default\": \"20\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]\n",
    "user_text = \"Fetch the first 15 comments for the IBM video with ID 456789123.\"\n",
    "response_text = json.dumps([\n",
    "  {\n",
    "    \"name\": \"comment_list\",\n",
    "    \"arguments\": {\n",
    "      \"video_id\": 456789123,\n",
    "      \"count\": 15\n",
    "    }\n",
    "  }\n",
    "])\n",
    "response_text = str(json.loads(response_text))\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_text}, {\"role\": \"assistant\", \"content\": response_text}]\n",
    "\n",
    "guardian_config = {\"criteria_id\": \"function_call\"}\n",
    "chat = tokenizer.apply_chat_template(messages, guardian_config = guardian_config, available_tools = tools, think=True, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "output = model.generate(chat, sampling_params, use_tqdm=False)\n",
    "response = output[0].outputs[0].text.strip()\n",
    "\n",
    "score, trace = parse_response(response)\n",
    "\n",
    "print(f\"# score: {score}\\n\")\n",
    "print(f\"# trace: {trace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### 3. Use Case: Bring your own Criteria\n",
    "\n",
    "You can bring your own criteria by passing `custom_criteria` and/or `custom_scoring_schema`\n",
    "\n",
    "Disclaimer: Note that the system may exhibit unexpected behaviors for new risk definitions. This could necessitate optimizing the definition, or even retraining the model.\n",
    "\n",
    "<details>\n",
    "    <summary>Code Hint</summary>\n",
    "    For custom criteria and/or custom scoring schema, pass <b><i>'custom_criteria'</i></b> and/or <b><i>'custom_scoring_schema'</i></b> argument through <b>'apply_chat_template' </b> method.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8a089-89b9-467a-8523-8c9bab1754e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = \"Hi, you can use my SSN : 123-1234-1245.\"\n",
    "custom_criteria = \"User message contains personal information or sensitive personal information that is included as a part of a prompt.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_text}]\n",
    "\n",
    "guardian_config = {\"custom_criteria\": custom_criteria}\n",
    "chat = tokenizer.apply_chat_template(messages, guardian_config = guardian_config, think=True, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "output = model.generate(chat, sampling_params, use_tqdm=False)\n",
    "response = output[0].outputs[0].text.strip()\n",
    "\n",
    "score, trace = parse_response(response)\n",
    "\n",
    "print(f\"# score: {score}\\n\")\n",
    "print(f\"# trace: {trace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7ed07-65e5-49d0-88f6-e65bcb7dbb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbea18-7b8b-4c35-b3af-7149a59d305b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite_guardian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
